{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def squareform_sp(w):\n",
    "#     \"\"\"\n",
    "#     Sparse counterpart of numpy's squareform\n",
    "    \n",
    "#     Parameters:\n",
    "#     w : sparse or dense vector with n(n-1)/2 elements OR matrix with size [n, n] and zero diagonal\n",
    "    \n",
    "#     Returns:\n",
    "#     W : matrix form of input vector w OR vector form of input matrix W\n",
    "#     \"\"\"\n",
    "#     if sp.issparse(w):\n",
    "#         is_sparse = True\n",
    "#     else:\n",
    "#         is_sparse = False\n",
    "#         w = np.asarray(w)\n",
    "    \n",
    "#     # Determine if input is a vector\n",
    "#     if w.ndim == 1 or w.shape[0] == 1 or w.shape[1] == 1:\n",
    "#         # VECTOR -> MATRIX\n",
    "#         l = w.size\n",
    "#         n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))\n",
    "#         if l != n * (n - 1) // 2:\n",
    "#             raise ValueError(\"Bad vector size!\")\n",
    "        \n",
    "#         if is_sparse:\n",
    "#             ind_vec = w.nonzero()[0]\n",
    "#             s = w.data\n",
    "#         else:\n",
    "#             ind_vec = np.nonzero(w)[0]\n",
    "#             s = w[ind_vec]\n",
    "        \n",
    "#         num_nz = len(ind_vec)\n",
    "#         ind_i = np.zeros(num_nz, dtype=int)\n",
    "#         ind_j = np.zeros(num_nz, dtype=int)\n",
    "        \n",
    "#         curr_row = 0\n",
    "#         offset = 0\n",
    "#         len_row = n - 1\n",
    "#         for idx in range(num_nz):\n",
    "#             ind_vec_i = ind_vec[idx]\n",
    "#             while ind_vec_i >= (len_row + offset):\n",
    "#                 offset += len_row\n",
    "#                 len_row -= 1\n",
    "#                 curr_row += 1\n",
    "#             ind_i[idx] = curr_row\n",
    "#             ind_j[idx] = ind_vec_i - offset + curr_row + 1\n",
    "        \n",
    "#         # For the lower triangular part, add the transposed matrix\n",
    "#         data = np.concatenate([s, s])\n",
    "#         row_indices = np.concatenate([ind_i, ind_j])\n",
    "#         col_indices = np.concatenate([ind_j, ind_i])\n",
    "#         W = sp.csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "#         return W\n",
    "\n",
    "#     else:\n",
    "#         # MATRIX -> VECTOR\n",
    "#         m, n = w.shape\n",
    "#         if m != n or not np.all(w.diagonal() == 0):\n",
    "#             raise ValueError(\"Matrix has to be square with zero diagonal!\")\n",
    "        \n",
    "#         if is_sparse:\n",
    "#             # Convert to COO format to align data and indices\n",
    "#             w_coo = w.tocoo()\n",
    "#             ind_i = w_coo.row\n",
    "#             ind_j = w_coo.col\n",
    "#             s = w_coo.data\n",
    "#         else:\n",
    "#             ind_i, ind_j = np.nonzero(w)\n",
    "#             s = w[ind_i, ind_j]\n",
    "        \n",
    "#         # Keep only upper triangular part\n",
    "#         ind_upper = ind_i < ind_j\n",
    "#         ind_i = ind_i[ind_upper]\n",
    "#         ind_j = ind_j[ind_upper]\n",
    "#         s = s[ind_upper]\n",
    "        \n",
    "#         # Compute new (vector) index from (i,j) (matrix) indices\n",
    "#         new_ind = n * ind_i - ind_i * (ind_i + 1) // 2 + ind_j - ind_i - 1\n",
    "#         l = n * (n - 1) // 2\n",
    "#         w_vec = sp.csr_matrix((s, (new_ind, np.zeros_like(new_ind))), shape=(l, 1))\n",
    "#         return w_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sum_squareform(n, mask=None):\n",
    "#     import numpy as np\n",
    "#     from scipy.sparse import csr_matrix\n",
    "\n",
    "#     if mask is not None:\n",
    "#         mask = np.asarray(mask).flatten()\n",
    "#         if len(mask) != n * (n - 1) // 2:\n",
    "#             raise ValueError('Mask size has to be n(n-1)/2')\n",
    "\n",
    "#         ind_vec = np.flatnonzero(mask)\n",
    "#         ncols = len(ind_vec)\n",
    "\n",
    "#         I = np.zeros(ncols, dtype=int)\n",
    "#         J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "#         curr_row = 0\n",
    "#         offset = 0\n",
    "#         len_row = n - 1\n",
    "#         for ii in range(ncols):\n",
    "#             ind_vec_i = ind_vec[ii]\n",
    "#             while ind_vec_i > (len_row + offset - 1):\n",
    "#                 offset += len_row\n",
    "#                 len_row -= 1\n",
    "#                 curr_row += 1\n",
    "#             I[ii] = curr_row\n",
    "#             J[ii] = ind_vec_i - offset + curr_row + 1\n",
    "#     else:\n",
    "#         ncols = n * (n - 1) // 2\n",
    "#         I = np.zeros(ncols, dtype=int)\n",
    "#         J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "#         k = 0\n",
    "#         for i in range(n - 1):\n",
    "#             for j in range(i + 1, n):\n",
    "#                 I[k] = i\n",
    "#                 J[k] = j\n",
    "#                 k += 1\n",
    "\n",
    "#     # Construct St\n",
    "#     row_indices = np.concatenate([np.arange(ncols), np.arange(ncols)])\n",
    "#     col_indices = np.concatenate([I, J])\n",
    "#     data = np.ones(2 * ncols)\n",
    "\n",
    "#     St = csr_matrix((data, (row_indices, col_indices)), shape=(ncols, n))\n",
    "#     S = St.transpose()\n",
    "\n",
    "#     return S, St\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python test\n",
    "# import numpy as np\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# # Sample symmetric matrix with zeros on the diagonal\n",
    "# n = 5\n",
    "# W = sp.random(n, n, density=0.2)\n",
    "# W = W + W.T  # Make it symmetric\n",
    "# W.setdiag(0)  # Set diagonal to zero\n",
    "\n",
    "# # Convert matrix to vector\n",
    "# w_vec = squareform_sp(W)\n",
    "\n",
    "# # Convert vector back to matrix\n",
    "# W_reconstructed = squareform_sp(w_vec)\n",
    "\n",
    "# # Verify that W and W_reconstructed are the same\n",
    "# print( (W != W_reconstructed).nnz == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.sparse import csr_matrix\n",
    "\n",
    "# # Define parameters\n",
    "# n = 4\n",
    "# mask = None  # or np.random.randint(0, 2, size=n * (n - 1) // 2)\n",
    "\n",
    "# # Generate random weights\n",
    "# W = np.random.rand(n, n)\n",
    "# W = (W + W.T) / 2  # Make it symmetric\n",
    "# np.fill_diagonal(W, 0)\n",
    "\n",
    "# # Vectorize W using squareform\n",
    "# from scipy.spatial.distance import squareform\n",
    "# w = squareform(W)\n",
    "\n",
    "# # Get S and St\n",
    "# S, St = sum_squareform(n, mask)\n",
    "\n",
    "# # Compute sum of weights connected to each node\n",
    "# node_sums = S @ w\n",
    "\n",
    "# # Compare with actual sums\n",
    "# actual_sums = W.sum(axis=1)\n",
    "\n",
    "# # Verify they are close\n",
    "# print( np.allclose(node_sums, actual_sums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "#     \"\"\"\n",
    "#     Learns a graph structure by optimizing a log-degrees model.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     Z : array-like\n",
    "#         Input data matrix or condensed distance matrix (squared pairwise distances).\n",
    "\n",
    "#     a : float\n",
    "#         Coefficient for the logarithmic penalty term (controls connectivity).\n",
    "\n",
    "#     b : float\n",
    "#         Regularization coefficient (controls sparsity).\n",
    "\n",
    "#     params : dict, optional\n",
    "#         A dictionary of optional parameters:\n",
    "#         - 'verbosity': int, level of verbosity (default: 1)\n",
    "#         - 'maxit': int, maximum number of iterations (default: 1000)\n",
    "#         - 'tol': float, tolerance for convergence (default: 1e-5)\n",
    "#         - 'step_size': float, step size for the gradient descent (default: 0.5)\n",
    "#         - 'max_w': float, maximum allowable weight (default: np.inf)\n",
    "#         - 'w_0': array-like or int, initial weight matrix or value (default: 0)\n",
    "#         - 'c': float, regularization coefficient for initial weight matrix\n",
    "#         - 'fix_zeros': bool, whether to fix zeros in the weight matrix (default: False)\n",
    "#         - 'edge_mask': array-like, mask for fixed edges (required if 'fix_zeros' is True)\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     W : array-like\n",
    "#         The learned weight matrix (n x n).\n",
    "\n",
    "#     stat : dict\n",
    "#         Output statistics including convergence information.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If 'params.w_0' is specified but 'params.c' is not, or if 'params.edge_mask' is required but not provided.\n",
    "#     \"\"\"\n",
    "#     # Default parameters\n",
    "#     if params is None:\n",
    "#         params = {}\n",
    "\n",
    "#     verbosity = params.get('verbosity', 1)\n",
    "#     maxit = params.get('maxit', 1000)\n",
    "#     tol = params.get('tol', 1e-5)\n",
    "#     step_size = params.get('step_size', 0.5)\n",
    "#     fix_zeros = params.get('fix_zeros', False)\n",
    "#     max_w = params.get('max_w', np.inf)\n",
    "#     w_0 = params.get('w_0', 0)\n",
    "\n",
    "#     # Initialize z\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         z = Z.copy()\n",
    "#     else:\n",
    "#         z = squareform_sp(Z)\n",
    "\n",
    "#     z = z.toarray().flatten()\n",
    "#     z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "#     l = len(z)\n",
    "#     n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "#     # Handle w_0\n",
    "#     if not np.isscalar(w_0) or w_0 != 0:\n",
    "#         if 'c' not in params:\n",
    "#             raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "#         else:\n",
    "#             c = params['c']\n",
    "#         if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "#             w_0 = w_0.copy()\n",
    "#         else:\n",
    "#             w_0 = squareform_sp(w_0)\n",
    "#         w_0 = w_0.flatten()\n",
    "#         w_0 = np.asarray(w_0, dtype=float)\n",
    "#     else:\n",
    "#         w_0 = 0\n",
    "#         c = 0  # Ensure c is defined\n",
    "\n",
    "#     # Handle fix_zeros\n",
    "#     if fix_zeros:\n",
    "#         if 'edge_mask' not in params:\n",
    "#             raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "#         edge_mask = params['edge_mask']\n",
    "#         if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "#             edge_mask = squareform_sp(edge_mask)\n",
    "#         ind = np.nonzero(edge_mask.flatten())[0]\n",
    "#         z = z[ind]\n",
    "#         if not np.isscalar(w_0):\n",
    "#             w_0 = w_0[ind]\n",
    "#     else:\n",
    "#         edge_mask = None\n",
    "\n",
    "#     # Initialize w\n",
    "#     w = np.zeros_like(z)\n",
    "\n",
    "#     # Needed operators\n",
    "#     if fix_zeros:\n",
    "#         S, St = sum_squareform(n, edge_mask)\n",
    "#     else:\n",
    "#         S, St = sum_squareform(n)\n",
    "\n",
    "#     K_op = lambda w: S @ w\n",
    "#     Kt_op = lambda z: St @ z\n",
    "\n",
    "#     if fix_zeros:\n",
    "#         norm_K = sparse_norm(S, ord=2)\n",
    "#     else:\n",
    "#         norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "#     # Define functions f, g, h\n",
    "#     f_eval = lambda w: 2 * np.dot(w, z)\n",
    "#     f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "#     param_prox_log = {'verbose': verbosity - 3}\n",
    "#     g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "#     # g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     # g_star_prox = lambda z_in, c_in: np.asarray(z_in) - c_in * a * prox_sum_log(np.asarray(z_in) / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     g_star_prox = lambda z, c: z - c * a * prox_sum_log(z / (c * a), 1 / (c * a), param_prox_log)[0]\n",
    "\n",
    "\n",
    "#     # Corrected h_eval and h_grad with division by 2\n",
    "#     if np.all(w_0 == 0):\n",
    "#         # No prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2\n",
    "#         h_grad = lambda w: b * w\n",
    "#         h_beta = b  # Lipschitz constant of h_grad\n",
    "#     else:\n",
    "#         # With prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2 + (c / 2) * np.linalg.norm(w - w_0) ** 2\n",
    "#         h_grad = lambda w: b * w + c * (w - w_0)\n",
    "#         h_beta = b + c  # Lipschitz constant of h_grad\n",
    "\n",
    "#     # Parameters for convergence\n",
    "#     mu = h_beta + norm_K\n",
    "#     epsilon = 1e-6  # A small positive value\n",
    "#     gn = (1 - epsilon) / mu  # Step size in (epsilon, (1 - epsilon)/mu)\n",
    "\n",
    "#     # Initialize variables\n",
    "#     v_n = K_op(w)\n",
    "\n",
    "#     stat = {}\n",
    "#     if verbosity > 1:\n",
    "#         stat['f_eval'] = []\n",
    "#         stat['g_eval'] = []\n",
    "#         stat['h_eval'] = []\n",
    "#         stat['fgh_eval'] = []\n",
    "#         stat['pos_violation'] = []\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Iterative optimization\n",
    "#     for i in range(maxit):\n",
    "#         # Primal and dual updates\n",
    "#         Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "#         y_n = v_n + gn * K_op(w)\n",
    "#         P_n = f_prox(Y_n, gn)\n",
    "#         p_n = g_star_prox(y_n, gn)\n",
    "#         Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "#         q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "#         # Compute relative norms\n",
    "#         rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "#         rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "#         if verbosity > 1:\n",
    "#             # Record statistics\n",
    "#             stat['f_eval'].append(f_eval(w))\n",
    "#             stat['g_eval'].append(g_eval(K_op(w)))\n",
    "#             stat['h_eval'].append(h_eval(w))\n",
    "#             stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "#             stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "#             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "\n",
    "#         # Update variables\n",
    "#         w = w - Y_n + Q_n\n",
    "#         v_n = v_n - y_n + q_n\n",
    "\n",
    "#         # Check convergence\n",
    "#         if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "#             break\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     if verbosity > 0:\n",
    "#         final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "#         print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "#         print(f'Time needed is {total_time} seconds')\n",
    "\n",
    "#     # Reconstruct full weight vector if fix_zeros was used\n",
    "#     if fix_zeros:\n",
    "#         w_full = np.zeros(l)\n",
    "#         w_full[ind] = w\n",
    "#         w = w_full\n",
    "\n",
    "#     # Convert vectorized weights back to matrix form\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         W = w\n",
    "#     else:\n",
    "#         W = squareform(w)\n",
    "\n",
    "#     stat['time'] = total_time\n",
    "\n",
    "#     return W, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def squareform_sp(Z):\n",
    "#     \"\"\"\n",
    "#     Converts a distance matrix Z into a vector form (or vice versa) similar to MATLAB's squareform,\n",
    "#     but returns a sparse matrix.\n",
    "#     \"\"\"\n",
    "#     if isinstance(Z, np.ndarray):\n",
    "#         # Assuming Z is a square matrix, extract the upper triangular part excluding the diagonal\n",
    "#         triu_indices = np.triu_indices_from(Z, k=1)\n",
    "#         data = Z[triu_indices]\n",
    "#         return coo_matrix((data, triu_indices), shape=Z.shape)\n",
    "#     else:\n",
    "#         # If Z is already a vector, return it as is\n",
    "#         return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "\n",
    "#     # Default parameters\n",
    "#     if params is None:\n",
    "#         params = {}\n",
    "\n",
    "#     verbosity = params.get('verbosity', 1)\n",
    "#     maxit = params.get('maxit', 1000)\n",
    "#     tol = params.get('tol', 1e-5)\n",
    "#     step_size = params.get('step_size', 0.5)  # Should be in (0, 1)\n",
    "#     fix_zeros = params.get('fix_zeros', False)\n",
    "#     max_w = params.get('max_w', np.inf)\n",
    "#     w_0 = params.get('w_0', 0)\n",
    "\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         z = Z.copy()\n",
    "#     else:\n",
    "#         z = squareform_sp(Z)\n",
    "    \n",
    "#     # Ensure z is a dense NumPy array\n",
    "#     if isinstance(z, np.ndarray):\n",
    "#         z = z.flatten()\n",
    "#     else:\n",
    "#         z = z.toarray().flatten()\n",
    "        \n",
    "#     z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "#     l = len(z)\n",
    "#     n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "#     # Handle w_0\n",
    "#     if not np.isscalar(w_0) or w_0 != 0:\n",
    "#         if 'c' not in params:\n",
    "#             raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "#         else:\n",
    "#             c = params['c']\n",
    "#         if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "#             w_0 = w_0.copy()\n",
    "#         else:\n",
    "#             w_0 = squareform_sp(w_0)\n",
    "#         w_0 = w_0.flatten()\n",
    "#         w_0 = np.asarray(w_0, dtype=float)\n",
    "#     else:\n",
    "#         w_0 = 0\n",
    "#         c = 0  # Ensure c is defined\n",
    "\n",
    "#     # Handle fix_zeros\n",
    "#     if fix_zeros:\n",
    "#         if 'edge_mask' not in params:\n",
    "#             raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "#         edge_mask = params['edge_mask']\n",
    "#         if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "#             edge_mask = squareform_sp(edge_mask)\n",
    "#         ind = np.nonzero(edge_mask.flatten())[0]\n",
    "#         z = z[ind]\n",
    "#         if not np.isscalar(w_0):\n",
    "#             w_0 = w_0[ind]\n",
    "#     else:\n",
    "#         edge_mask = None\n",
    "\n",
    "#     # Initialize w\n",
    "#     w = np.zeros_like(z)\n",
    "\n",
    "#     # Needed operators\n",
    "#     if fix_zeros:\n",
    "#         S, St = sum_squareform(n, edge_mask)\n",
    "#     else:\n",
    "#         S, St = sum_squareform(n)\n",
    "\n",
    "#     K_op = lambda w: S @ w\n",
    "#     Kt_op = lambda z: St @ z\n",
    "\n",
    "#     if fix_zeros:\n",
    "#         norm_K = sparse_norm(S, ord=2)\n",
    "#     else:\n",
    "#         norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "#     # Define functions f, g, h\n",
    "#     f_eval = lambda w: 2 * np.dot(w, z)\n",
    "#     f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "#     param_prox_log = {'verbose': verbosity - 3}\n",
    "#     g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "#     g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)[0]\n",
    "\n",
    "#     # Corrected h_eval and h_grad\n",
    "#     if np.all(w_0 == 0):\n",
    "#         # No prior W0\n",
    "#         h_eval = lambda w: b * np.linalg.norm(w) ** 2\n",
    "#         h_grad = lambda w: 2 * b * w\n",
    "#         h_beta = 2 * b\n",
    "#     else:\n",
    "#         # With prior W0\n",
    "#         h_eval = lambda w: b * np.linalg.norm(w) ** 2 + c * np.linalg.norm(w - w_0) ** 2\n",
    "#         h_grad = lambda w: 2 * b * w + 2 * c * (w - w_0)\n",
    "#         h_beta = 2 * (b + c)\n",
    "\n",
    "#     # Parameters for convergence\n",
    "#     mu = h_beta + norm_K\n",
    "#     gn = step_size / mu  # Step size in (0, 1/mu)\n",
    "\n",
    "#     # Initialize variables\n",
    "#     v_n = K_op(w)\n",
    "\n",
    "#     stat = {}\n",
    "#     if verbosity > 1:\n",
    "#         stat['f_eval'] = []\n",
    "#         stat['g_eval'] = []\n",
    "#         stat['h_eval'] = []\n",
    "#         stat['fgh_eval'] = []\n",
    "#         stat['pos_violation'] = []\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Iterative optimization\n",
    "#     for i in range(maxit):\n",
    "#         # Primal and dual updates\n",
    "#         Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "#         y_n = v_n + gn * K_op(w)\n",
    "#         P_n = f_prox(Y_n, gn)\n",
    "#         p_n = g_star_prox(y_n, gn)\n",
    "#         Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "#         q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "#         # Compute relative norms\n",
    "#         rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "#         rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "#         if verbosity > 1:\n",
    "#             # Record statistics\n",
    "#             stat['f_eval'].append(f_eval(w))\n",
    "#             stat['g_eval'].append(g_eval(K_op(w)))\n",
    "#             stat['h_eval'].append(h_eval(w))\n",
    "#             stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "#             stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "#             if verbosity > 2:\n",
    "#                 print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "#             else:\n",
    "#                 print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}')\n",
    "\n",
    "#         # Update variables\n",
    "#         w = w - Y_n + Q_n\n",
    "#         v_n = v_n - y_n + q_n\n",
    "\n",
    "#         # Check convergence\n",
    "#         if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "#             break\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     if verbosity > 0:\n",
    "#         final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "#         print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "#         print(f'Time needed is {total_time:.4f} seconds')\n",
    "\n",
    "#     # Reconstruct full weight vector if fix_zeros was used\n",
    "#     if fix_zeros:\n",
    "#         w_full = np.zeros(l)\n",
    "#         w_full[ind] = w\n",
    "#         w = w_full\n",
    "\n",
    "#     # Convert vectorized weights back to matrix form\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         W = w\n",
    "#     else:\n",
    "#         W = squareform(w)\n",
    "\n",
    "#     stat['time'] = total_time\n",
    "\n",
    "#     return W, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "    \"\"\"\n",
    "    Python version of the MATLAB function gsp_learn_graph_log_degrees.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    verbosity = params.get('verbosity', 1)\n",
    "    maxit = params.get('maxit', 1000)\n",
    "    tol = params.get('tol', 1e-5)\n",
    "    step_size = params.get('step_size', 0.5)\n",
    "    fix_zeros = params.get('fix_zeros', False)\n",
    "    max_w = params.get('max_w', np.inf)\n",
    "\n",
    "    # Handle w_0 and c if provided\n",
    "    w_0 = params.get('w_0', None)\n",
    "    c = params.get('c', 0.0 if w_0 is None else None)\n",
    "    if w_0 is not None and c is None:\n",
    "        raise ValueError('When params.w_0 is specified, params.c should also be specified')\n",
    "    \n",
    "    # Convert Z to the vector form z\n",
    "    if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "        # Z is n-by-n\n",
    "        z = squareform_sp(Z)\n",
    "    else:\n",
    "        # Z is a vector\n",
    "        z = Z\n",
    "    z = z.toarray().flatten()\n",
    "    l = len(z)\n",
    "    n = int(round((1 + np.sqrt(1+8*l))/2))\n",
    "\n",
    "    if w_0 is None:\n",
    "        w_0 = 0\n",
    "        c = 0.0\n",
    "    else:\n",
    "        if w_0.ndim == 2 and w_0.shape[0] == w_0.shape[1]:\n",
    "            w_0 = squareform_sp(w_0)\n",
    "        w_0 = w_0.flatten()\n",
    "\n",
    "    # If fix_zeros is set, use only a subset of edges\n",
    "    if fix_zeros:\n",
    "        edge_mask = params.get('edge_mask', None)\n",
    "        if edge_mask is None:\n",
    "            raise ValueError('edge_mask must be provided when fix_zeros is True')\n",
    "        if edge_mask.ndim == 2 and edge_mask.shape[0] == edge_mask.shape[1]:\n",
    "            edge_mask = squareform_sp(edge_mask)\n",
    "        edge_mask = edge_mask.flatten()\n",
    "        ind = np.where(edge_mask != 0)[0]\n",
    "        z = z[ind]\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0[ind]\n",
    "    else:\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0\n",
    "        else:\n",
    "            w_0 = np.array([w_0]*l)\n",
    "\n",
    "    z = z.astype(float)\n",
    "    w_0 = w_0.astype(float) if w_0.size == l else w_0\n",
    "    w = np.zeros_like(z)\n",
    "\n",
    "    # Construct operators S and St\n",
    "    if fix_zeros:\n",
    "        S, St = sum_squareform(n, edge_mask)\n",
    "    else:\n",
    "        S, St = sum_squareform(n)\n",
    "\n",
    "    # Operator K_op = S*w and Kt_op = St*z\n",
    "    def K_op(w_):\n",
    "        return S.dot(w_)\n",
    "    def Kt_op(z_):\n",
    "        return St.dot(z_)\n",
    "\n",
    "    # Norm estimate of K\n",
    "    # If fix_zeros: norm_K is approximated by normest(S)\n",
    "    # Otherwise norm_K = sqrt(2*(n-1))\n",
    "    if fix_zeros:\n",
    "        # normest: a rough estimation can be done by power method:\n",
    "        # For simplicity, we do a quick estimate\n",
    "        # (This is a minor approximation; for full fidelity, implement a proper normest.)\n",
    "        # We'll just do a few power iterations:\n",
    "        x = np.random.randn(S.shape[1])\n",
    "        for _ in range(5):\n",
    "            x = St.dot(S.dot(x))\n",
    "            norm_est = np.linalg.norm(x)\n",
    "            if norm_est == 0:\n",
    "                break\n",
    "            x /= norm_est\n",
    "        norm_K = np.sqrt(norm_est)\n",
    "    else:\n",
    "        norm_K = np.sqrt(2*(n-1))\n",
    "\n",
    "    # Define function handles f, g, h\n",
    "    # f: deals with data fidelity: f.eval = 2*w'z, f.prox = ...\n",
    "    def f_eval(w_):\n",
    "        return 2*np.dot(w_, z)\n",
    "    def f_prox(w_, c_):\n",
    "        return np.minimum(max_w, np.maximum(0, w_ - 2*c_*z))\n",
    "\n",
    "    param_prox_log = {}  # not used extensively here\n",
    "    def g_eval(x):\n",
    "        return -a * np.sum(np.log(x))\n",
    "    def g_prox(z_, c_):\n",
    "        # prox of g w.r.t. z_ at parameter c: g(z) = -a sum(log(z))\n",
    "        # prox_sum_log solves the subproblem coordinate-wise\n",
    "        return prox_sum_log(z_, c_*a, param_prox_log)\n",
    "\n",
    "    # g_star_prox used in the algorithm:\n",
    "    # g_star_prox(z,c) = z - c*a * prox_sum_log(z/(c*a), 1/(c*a))\n",
    "    def g_star_prox(z_, c_):\n",
    "        return z_ - c_*a * prox_sum_log(z_/(c_*a), 1/(c_*a), param_prox_log)\n",
    "\n",
    "    if c == 0:\n",
    "        def h_eval(w_):\n",
    "            return b * np.sum(w_**2)\n",
    "        def h_grad(w_):\n",
    "            return 2*b*w_\n",
    "        h_beta = 2*b\n",
    "    else:\n",
    "        def h_eval(w_):\n",
    "            return b*np.sum(w_**2) + c*np.sum((w_-w_0)**2)\n",
    "        def h_grad(w_):\n",
    "            return 2*((b+c)*w_ - c*w_0)\n",
    "        h_beta = 2*(b+c)\n",
    "\n",
    "    mu = h_beta + norm_K\n",
    "    epsilon = lin_map(0.0, [0, 1/(1+mu)], [0,1])\n",
    "    gn = lin_map(step_size, [epsilon, (1-epsilon)/mu], [0,1])\n",
    "\n",
    "    stat = {}\n",
    "    if verbosity > 1 or True:\n",
    "        stat['f_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['g_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['h_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['fgh_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['pos_violation'] = np.zeros(maxit)*np.nan\n",
    "\n",
    "    v_n = K_op(w)\n",
    "\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(maxit):\n",
    "        Y_n = w - gn*(h_grad(w) + Kt_op(v_n))\n",
    "        y_n = v_n + gn*(K_op(w))\n",
    "\n",
    "        P_n = f_prox(Y_n, gn)\n",
    "        p_n = g_star_prox(y_n, gn)\n",
    "\n",
    "        Q_n = P_n - gn*(h_grad(P_n) + Kt_op(p_n))\n",
    "        q_n = p_n + gn*(K_op(P_n))\n",
    "\n",
    "        if verbosity > 2 or True:\n",
    "            stat['f_eval'][i] = f_eval(w)\n",
    "            Kw = K_op(w)\n",
    "            stat['g_eval'][i] = g_eval(Kw)\n",
    "            stat['h_eval'][i] = h_eval(w)\n",
    "            stat['fgh_eval'][i] = stat['f_eval'][i] + stat['g_eval'][i] + stat['h_eval'][i]\n",
    "            stat['pos_violation'][i] = -np.sum(np.minimum(0,w))\n",
    "\n",
    "        rel_norm_primal = np.linalg.norm(- Y_n + Q_n)/max(1e-15, np.linalg.norm(w))\n",
    "        rel_norm_dual = np.linalg.norm(- y_n + q_n)/max(1e-15, np.linalg.norm(v_n))\n",
    "\n",
    "        if verbosity > 2:\n",
    "            print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   {stat[\"fgh_eval\"][i]:6.3e}')\n",
    "        elif verbosity > 1:\n",
    "            print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}')\n",
    "\n",
    "        w = w - Y_n + Q_n\n",
    "        v_n = v_n - y_n + q_n\n",
    "\n",
    "        if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "            break\n",
    "\n",
    "    stat['time'] = time.time() - t0\n",
    "    if verbosity > 0:\n",
    "        print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {f_eval(w) + g_eval(K_op(w)) + h_eval(w):6.3e}')\n",
    "        print(f'Time needed is {stat[\"time\"]} seconds')\n",
    "\n",
    "    # Rebuild full w if fix_zeros\n",
    "    if fix_zeros:\n",
    "        # We must put the learned edges back into a vector of length l with zeros where edges are masked out.\n",
    "        full_w = np.zeros(l + len(ind)*0, dtype=float)  # original l from Z\n",
    "        full_w[ind] = w\n",
    "        w = full_w\n",
    "\n",
    "    # Convert w back to W if Z was a matrix\n",
    "    if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "        W = squareform_sp(w)\n",
    "    else:\n",
    "        W = w\n",
    "\n",
    "    return W, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def glmm(y, iterations, classes, spread=0.1, regul=0.15, norm_par=1.5):\n",
    "#     \"\"\"\n",
    "#     Implements a Graph Laplacian Mixture Model (GLMM) \n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     y : np.ndarray\n",
    "#         The input data matrix of shape (m, n) where m is the number of signals\n",
    "#         and n is the number of features.\n",
    "    \n",
    "#     iterations : int\n",
    "#         The number of iterations for the algorithm.\n",
    "    \n",
    "#     classes : int\n",
    "#         The number of clusters (classes).\n",
    "    \n",
    "#     spread : float, optional\n",
    "#         Spread parameter for initializing the Laplacian matrices (default: 0.1).\n",
    "    \n",
    "#     regul : float, optional\n",
    "#         Regularization parameter for the covariance matrices (default: 0.15).\n",
    "    \n",
    "#     norm_par : float, optional\n",
    "#         Normalization parameter for the distance computation (default: 1.5).\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     L : np.ndarray\n",
    "#         The learned Laplacian matrices of shape (n, n, classes).\n",
    "    \n",
    "#     gamma_hat : np.ndarray\n",
    "#         The cluster probabilities of shape (m, classes).\n",
    "    \n",
    "#     mu : np.ndarray\n",
    "#         The cluster means of shape (n, classes).\n",
    "    \n",
    "#     log_likelihood : np.ndarray\n",
    "#         The log-likelihood values over the iterations.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If the dimensions of the input data are not correct.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     delta = 2\n",
    "#     n = y.shape[1]\n",
    "#     m = y.shape[0]\n",
    "\n",
    "#     # Initialize variables\n",
    "#     L = np.zeros((n, n, classes))\n",
    "#     W = np.zeros((n, n, classes))\n",
    "#     sigma = np.zeros((n-1, n-1, classes))\n",
    "#     mu = np.zeros((n, classes))\n",
    "#     gamma_hat = np.zeros((m, classes))\n",
    "#     p = np.zeros(classes)\n",
    "#     vecl = np.zeros((n, n, classes))\n",
    "#     vall = np.zeros((n, n, classes))\n",
    "#     yl = np.zeros((m, n-1, classes))\n",
    "#     log_likelihood = np.zeros(iterations)\n",
    "\n",
    "#     for cls in range(classes):\n",
    "#         L[:, :, cls] = spread * np.eye(n) - spread / n * np.ones((n, n))\n",
    "#         mu_curr = np.mean(y, axis=0) + np.random.randn(n) * np.std(y, axis=0)\n",
    "#         mu[:, cls] = mu_curr - np.mean(mu_curr)\n",
    "#         p[cls] = 1 / classes\n",
    "\n",
    "#     # Start the algorithm\n",
    "#     for it in range(iterations):\n",
    "#         # Expectation step\n",
    "#         pall = 0\n",
    "#         for cls in range(classes):\n",
    "#             # vecl[:, :, cls], vall[:, :, cls] = eig(L[:, :, cls])\n",
    "#             eig_vals, eig_vecs = np.linalg.eig(L[:, :, cls])\n",
    "#             eig_vecs = np.real(eig_vecs)\n",
    "#             eig_vals = np.real(eig_vals)\n",
    "#             eig_vals[eig_vals < 0] = 1e-5\n",
    "#             vall[:, :, cls] = np.diag(eig_vals)\n",
    "#             vecl[:, :, cls] = eig_vecs\n",
    "#             sigma[:, :, cls] = inv(vall[1:n, 1:n, cls] + regul * np.eye(n-1))  \n",
    "#             sigma[:, :, cls] = (sigma[:, :, cls] + sigma[:, :, cls].T) / 2\n",
    "#             yl[:, :, cls] = (y - mu[:, cls].T) @ vecl[:, 1:n, cls]\n",
    "#             pall += p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls], allow_singular=True)\n",
    "\n",
    "#         # Compute cluster probabilities gamma_hat  \n",
    "#         pall[pall == 0] = 0.1\n",
    "#         for cls in range(classes):\n",
    "#             gamma_hat[:, cls] = (p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls])) / pall\n",
    "\n",
    "#         log_likelihood[it] = np.sum(np.log(pall))\n",
    "\n",
    "#         # Maximization step: update mu, W, and p\n",
    "#         for cls in range(classes):\n",
    "#             mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n",
    "#             # yc = repmat(np.sqrt(gamma_hat[:, cls])[:, np.newaxis], 1, n) * (y - mu[:, cls])\n",
    "#             yc = np.sqrt(gamma_hat[:, cls])[:, np.newaxis] * (y - mu[:, cls])\n",
    "#             Z = gsp_distanz(yc) ** 2\n",
    "#             theta = np.mean(Z) / norm_par\n",
    "#             W_curr = delta * (gsp_learn_graph_log_degrees(Z / theta, 1, 1))[0]\n",
    "#             W[:, :, cls] = W_curr\n",
    "#             p[cls] = np.sum(gamma_hat[:, cls]) / m\n",
    "#             # Compute Ls\n",
    "#             L[:, :, cls] = np.diag(np.sum(W[:, :, cls], axis=1)) - W[:, :, cls]\n",
    "#             W_curr[W_curr < 1e-3] = 0\n",
    "#             W[:, :, cls] = W_curr\n",
    "\n",
    "#     return L, gamma_hat, mu, log_likelihood\n",
    "\n",
    "# #   mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def prox_sum_log(x, gamma, param=None):\n",
    "#     \"\"\"\n",
    "#     Proximal operator of log-barrier - sum(log(x))\n",
    "\n",
    "#     Solves:\n",
    "    #     sol = argmin_{z} 0.5*||x - z||_2^2 - gamma * sum(log(z))\n",
    "\n",
    "    # Parameters:\n",
    "    # ----------\n",
    "    # x : array-like\n",
    "    #     Input signal (vector or matrix).\n",
    "    # gamma : float\n",
    "    #     Regularization parameter.\n",
    "    # param : dict, optional\n",
    "    #     Dictionary of optional parameters:\n",
    "    #     - 'verbose': int, verbosity level (default: 1).\n",
    "\n",
    "    # Returns:\n",
    "    # -------\n",
    "    # sol : numpy.ndarray\n",
    "    #     Solution to the optimization problem.\n",
    "    # info : dict\n",
    "    #     Dictionary summarizing information at convergence.\n",
    "    # \"\"\"\n",
    "    # if param is None:\n",
    "    #     param = {}\n",
    "    \n",
    "    # verbose = param.get('verbose', 1)\n",
    "    \n",
    "    # if gamma < 0:\n",
    "    #     raise ValueError('Gamma cannot be negative')\n",
    "    # elif gamma == 0:\n",
    "    #     stop_error = True\n",
    "    # else:\n",
    "    #     stop_error = False\n",
    "    \n",
    "    # t1 = time.time()\n",
    "    \n",
    "    # if stop_error:\n",
    "    #     sol = x\n",
    "    #     info = {\n",
    "    #         'algo': 'prox_sum_log',\n",
    "    #         'iter': 0,\n",
    "    #         'final_eval': 0,\n",
    "    #         'crit': '--',\n",
    "    #         'time': time.time() - t1\n",
    "    #     }\n",
    "    #     return sol, info\n",
    "    \n",
    "    # Compute the solution\n",
    "    # sol = (x + np.sqrt(x**2 + 4*gamma)) / 2\n",
    "    \n",
    "    # # Compute the final evaluation of the function at the solution\n",
    "    # final_eval = 0.5 * np.sum((x - sol)**2) - gamma * np.sum(np.log(sol.flatten()))\n",
    "    \n",
    "    # info = {\n",
    "    #     'algo': 'prox_sum_log',\n",
    "    #     'iter': 0,\n",
    "    #     'final_eval': final_eval,\n",
    "    #     'crit': '--',\n",
    "    #     'time': time.time() - t1\n",
    "    # }\n",
    "    \n",
    "    # # Verbose output\n",
    "    # if verbose >= 1:\n",
    "    #     print(f'  prox_sum_log: final evaluation = {info[\"final_eval\"]:.6e}')\n",
    "    #     if verbose > 1:\n",
    "    #         n_neg = np.sum(sol.flatten() <= 0)\n",
    "    #         if n_neg > 0:\n",
    "    #             print(f' ({n_neg} negative elements in solution, log not defined, check stability)')\n",
    "    #     print()\n",
    "    \n",
    "    # return sol, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def sum_squareform(n, mask=None):\n",
    "#     \"\"\"\n",
    "#     Computes the sum and transpose sum matrices in a squareform format.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     n : int\n",
    "#         The size of the squareform matrix.\n",
    "    \n",
    "#     mask : array-like, optional\n",
    "#         A mask to filter the indices. The length of the mask must be n(n-1)/2.\n",
    "#         If provided, only the elements corresponding to the non-zero values in\n",
    "#         the mask are considered.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     S : csr_matrix\n",
    "#         A sparse matrix so that S * w = sum(W), where w = squareform(W)\n",
    "    \n",
    "#     St : csr_matrix\n",
    "#         The adjoint (transpose) of S.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If the length of the mask is not equal to n(n-1)/2.\n",
    "#     \"\"\"\n",
    "#     if mask is not None:\n",
    "#         mask = np.asarray(mask).flatten()\n",
    "#         if len(mask) != n * (n - 1) // 2:\n",
    "#             raise ValueError('Mask size has to be n(n-1)/2')\n",
    "\n",
    "#         ind_vec = np.flatnonzero(mask)\n",
    "#         ncols = len(ind_vec)\n",
    "\n",
    "#         I = np.zeros(ncols, dtype=int)\n",
    "#         J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "#         curr_row = 0\n",
    "#         offset = 0\n",
    "#         len_row = n - 1\n",
    "#         for ii in range(ncols):\n",
    "#             ind_vec_i = ind_vec[ii]\n",
    "#             while ind_vec_i > (len_row + offset - 1):\n",
    "#                 offset += len_row\n",
    "#                 len_row -= 1\n",
    "#                 curr_row += 1\n",
    "#             I[ii] = curr_row\n",
    "#             J[ii] = ind_vec_i - offset + curr_row + 1\n",
    "#     else:\n",
    "#         ncols = n * (n - 1) // 2\n",
    "#         I = np.zeros(ncols, dtype=int)\n",
    "#         J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "#         k = 0\n",
    "#         for i in range(n - 1):\n",
    "#             for j in range(i + 1, n):\n",
    "#                 I[k] = i\n",
    "#                 J[k] = j\n",
    "#                 k += 1\n",
    "\n",
    "#     # Construct St\n",
    "#     row_indices = np.concatenate([np.arange(ncols), np.arange(ncols)])\n",
    "#     col_indices = np.concatenate([I, J])\n",
    "#     data = np.ones(2 * ncols)\n",
    "\n",
    "#     St = csr_matrix((data, (np.arange(2 * ncols), col_indices)), shape=(2 * ncols, n))\n",
    "#     St = csr_matrix((data, (row_indices, col_indices)), shape=(ncols, n))\n",
    "#     S = St.transpose()\n",
    "\n",
    "#     return S, St\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def squareform_sp(w):\n",
    "#     \"\"\"\n",
    "#     Sparse counterpart of numpy's squareform\n",
    "    \n",
    "#     Parameters:\n",
    "#     w : sparse or dense vector with n(n-1)/2 elements OR matrix with size [n, n] and zero diagonal\n",
    "    \n",
    "#     Returns:\n",
    "#     W : matrix form of input vector w OR vector form of input matrix W\n",
    "#     \"\"\"\n",
    "#     import numpy as np\n",
    "#     import scipy.sparse as sp\n",
    "#     from scipy.spatial.distance import squareform\n",
    "\n",
    "#     if sp.issparse(w):\n",
    "#         is_sparse = True\n",
    "#     else:\n",
    "#         is_sparse = False\n",
    "#         w = np.asarray(w)\n",
    "    \n",
    "#     # Determine if input is a vector\n",
    "#     if w.ndim == 1 or w.shape[0] == 1 or w.shape[1] == 1:\n",
    "#         # VECTOR -> MATRIX\n",
    "#         if w.ndim == 1:\n",
    "#             l = w.shape[0]\n",
    "#         else:\n",
    "#             l = w.shape[0] * w.shape[1]\n",
    "#         n = int(round((1 + np.sqrt(1 + 8*l)) / 2))\n",
    "        \n",
    "#         # Check input\n",
    "#         if l != n*(n-1)//2:\n",
    "#             raise ValueError(\"Bad vector size!\")\n",
    "        \n",
    "#         if is_sparse:\n",
    "#             ind_vec = w.nonzero()[0]\n",
    "#             s = w.data\n",
    "#         else:\n",
    "#             ind_vec = np.nonzero(w)[0]\n",
    "#             s = w[ind_vec]\n",
    "        \n",
    "#         num_nz = len(ind_vec)\n",
    "        \n",
    "#         ind_i = np.zeros(num_nz, dtype=int)\n",
    "#         ind_j = np.zeros(num_nz, dtype=int)\n",
    "        \n",
    "#         curr_row = 0\n",
    "#         offset = 0\n",
    "#         len_row = n - 1\n",
    "#         for idx in range(num_nz):\n",
    "#             ind_vec_i = ind_vec[idx]\n",
    "#             while ind_vec_i >= (len_row + offset):\n",
    "#                 offset += len_row\n",
    "#                 len_row -= 1\n",
    "#                 curr_row += 1\n",
    "#             ind_i[idx] = curr_row\n",
    "#             ind_j[idx] = ind_vec_i - offset + curr_row + 1\n",
    "        \n",
    "#         # For the lower triangular part, add the transposed matrix\n",
    "#         data = np.concatenate([s, s])\n",
    "#         row_indices = np.concatenate([ind_i, ind_j])\n",
    "#         col_indices = np.concatenate([ind_j, ind_i])\n",
    "#         W = sp.csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "#         return W\n",
    "\n",
    "#     else:\n",
    "#         # MATRIX -> VECTOR\n",
    "#         m, n = w.shape\n",
    "#         if m != n or not np.all(w.diagonal() == 0):\n",
    "#             raise ValueError(\"Matrix has to be square with zero diagonal!\")\n",
    "        \n",
    "#         if is_sparse:\n",
    "#             ind_i, ind_j = w.nonzero()\n",
    "#             s = w.data\n",
    "#         else:\n",
    "#             ind_i, ind_j = np.nonzero(w)\n",
    "#             s = w[ind_i, ind_j]\n",
    "        \n",
    "#         # Keep only upper triangular part\n",
    "#         ind_upper = ind_i < ind_j\n",
    "#         ind_i = ind_i[ind_upper]\n",
    "#         ind_j = ind_j[ind_upper]\n",
    "#         s = s[ind_upper]\n",
    "        \n",
    "#         # Compute new (vector) index from (i,j) (matrix) indices\n",
    "#         new_ind = n * ind_i - ind_i * (ind_i + 1) // 2 + ind_j - ind_i - 1\n",
    "#         l = n * (n - 1) // 2\n",
    "#         w_vec = sp.csr_matrix((s, (new_ind, np.zeros_like(new_ind))), shape=(l, 1))\n",
    "#         return w_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gsp_distanz(X, Y=None, P=None):\n",
    "#     \"\"\"\n",
    "#     Calculates the distances between all vectors in X and Y using a provided matrix P for scaling (weighted distances).\n",
    "    \n",
    "#     Parameters:\n",
    "#         X (numpy.ndarray): Matrix with column vectors of shape (n_features, n_samples_X).\n",
    "#         Y (numpy.ndarray, optional): Matrix with column vectors of shape (n_features, n_samples_Y). Defaults to X.\n",
    "#         P (numpy.ndarray, optional): Weight matrix of shape (n_features, n_features). Defaults to the identity matrix.\n",
    "\n",
    "#     Returns:\n",
    "#         numpy.ndarray: Distance matrix of shape (n_samples_X, n_samples_Y), not squared.\n",
    "        \n",
    "#     Raises:\n",
    "#         ValueError: If the dimensions of X and Y do not match.\n",
    "#         ValueError: If the dimensions of P do not match the number of features in X.\n",
    "\n",
    "#     Usage:\n",
    "#         D = gsp_distanz(X, Y, P)\n",
    "        \n",
    "#     Notes:\n",
    "#         This function computes the following:\n",
    "        \n",
    "#             D = sqrt((X - Y)^T P (X - Y))\n",
    "        \n",
    "#         for all vectors in X and Y. If P is not provided, it defaults to the identity matrix, reducing the calculation to the Euclidean distance.\n",
    "#         The function is optimized for speed using vectorized operations, avoiding explicit loops.\n",
    "#     \"\"\"\n",
    "#     if Y is None:\n",
    "#         Y = X\n",
    "\n",
    "#     if X.shape[0] != Y.shape[0]:\n",
    "#         raise ValueError(\"The sizes of X and Y do not match!\")\n",
    "\n",
    "#     n_features, n_samples_X = X.shape\n",
    "#     _, n_samples_Y = Y.shape\n",
    "\n",
    "#     if P is None:\n",
    "#         xx = np.sum(X**2, axis=0)  # ||x||^2, shape (n_samples_X,)\n",
    "#         yy = np.sum(Y**2, axis=0)  # ||y||^2, shape (n_samples_Y,)\n",
    "#         xy = X.T @ Y               # x^T y, shape (n_samples_X, n_samples_Y)\n",
    "#         D = np.abs(np.add.outer(xx, yy) - 2 * xy)\n",
    "#     else:\n",
    "#         rp, rp2 = P.shape\n",
    "#         if n_features != rp or rp != rp2:\n",
    "#             raise ValueError(\"P must be square and match the dimension of X!\")\n",
    "\n",
    "#         # Compute x^T P x and y^T P y\n",
    "#         xx = np.sum(X * (P @ X), axis=0)  # shape (n_samples_X,)\n",
    "#         yy = np.sum(Y * (P @ Y), axis=0)  # shape (n_samples_Y,)\n",
    "\n",
    "#         # Compute x^T P y and y^T P x\n",
    "#         xy = X.T @ (P @ Y)  # shape (n_samples_X, n_samples_Y)\n",
    "#         yx = Y.T @ (P @ X)  # shape (n_samples_Y, n_samples_X)\n",
    "\n",
    "#         # D = |xx_i + yy_j - (x_i^T P y_j) - (y_j^T P x_i)|\n",
    "#         # Since yx.T has shape (n_samples_X, n_samples_Y), we can subtract it directly\n",
    "#         D = np.abs(np.add.outer(xx, yy) - xy - yx.T)\n",
    "\n",
    "#     # Check for negative values in D\n",
    "#     if np.any(D < 0):\n",
    "#         print('Warning: P is not semipositive or x is not real!')\n",
    "\n",
    "#     # Take the square root\n",
    "#     D = np.sqrt(D)\n",
    "\n",
    "#     # If Y is X, set the diagonal to zero\n",
    "#     if Y is X:\n",
    "#         np.fill_diagonal(D, 0)\n",
    "\n",
    "#     return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_learning_perf_eval(L_0, L):\n",
    "    # Evaluate the performance of graph learning algorithms\n",
    "    L_0tmp = L_0 - np.diag(np.diag(L_0))\n",
    "    edges_groundtruth = (squareform(L_0tmp) != 0)\n",
    "\n",
    "    Ltmp = L - np.diag(np.diag(L))\n",
    "    edges_learned = (squareform(Ltmp) != 0)\n",
    "\n",
    "    # Compute precision, recall\n",
    "    # True Positive (TP): edge is in groundtruth and learned\n",
    "    TP = np.sum(edges_learned & edges_groundtruth)\n",
    "    FP = np.sum(edges_learned & (~edges_groundtruth))\n",
    "    FN = np.sum((~edges_learned) & edges_groundtruth)\n",
    "\n",
    "    if (TP + FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    if (TP + FN) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "\n",
    "    # F-measure\n",
    "    if (precision + recall) == 0:\n",
    "        f = 0.0\n",
    "    else:\n",
    "        f = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # NMI - convert boolean arrays to integers\n",
    "    # NMI expects labels or cluster assignments. We'll treat edges as binary labels.\n",
    "    NMI_score = nmi(edges_learned.astype(int), edges_groundtruth.astype(int))\n",
    "    if np.isnan(NMI_score):\n",
    "        NMI_score = 0.0\n",
    "\n",
    "    # number of edges in the learned graph\n",
    "    num_of_edges = np.sum(edges_learned)\n",
    "\n",
    "    return precision, recall, f, NMI_score, num_of_edges\n",
    "\n",
    "\n",
    "def identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k):\n",
    "    identify = np.zeros(k, dtype=int)\n",
    "    cl_err = np.inf * np.ones(k)\n",
    "    for i in range(k):\n",
    "        # Make W from L\n",
    "        W = np.diag(np.diag(Ls[:, :, i])) - Ls[:, :, i]\n",
    "        W[W < 0.001] = 0\n",
    "        Ls[:, :, i] = np.diag(np.sum(W, axis=1)) - W\n",
    "        # Find best matching cluster j\n",
    "        for j in range(k):\n",
    "            er = np.linalg.norm(gamma_hats[:, i] - gamma_cut[:, j], 'fro')\n",
    "            if cl_err[i] > er:\n",
    "                cl_err[i] = er\n",
    "                identify[i] = j\n",
    "\n",
    "    precision = np.zeros((k, 1))\n",
    "    recall = np.zeros((k, 1))\n",
    "    f = np.zeros((k, 1))\n",
    "    for i in range(k):\n",
    "        p, r, ff, NMI_score, num_edges = graph_learning_perf_eval(Lap[:, :, identify[i]], Ls[:, :, i])\n",
    "        precision[i] = p\n",
    "        recall[i] = r\n",
    "        f[i] = ff\n",
    "        # NMI_score and num_edges are not stored as per original code except NMI in a local var.\n",
    "        # If needed, store them similarly.\n",
    "\n",
    "    cl_errors = np.diag((gamma_hats - gamma_cut[:, identify]) \\\n",
    "                        .T @ (gamma_hats - gamma_cut[:, identify]))\n",
    "    return identify, precision, recall, f, cl_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k):\n",
    "#     \"\"\"\n",
    "#     Identifies and compares clusters based on given Laplacian matrices and cluster assignments.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     Ls : np.ndarray\n",
    "#         Estimated Laplacian matrices of shape (n, n, k).\n",
    "    \n",
    "#     Lap : np.ndarray\n",
    "#         Ground truth Laplacian matrices of shape (n, n, k).\n",
    "    \n",
    "#     gamma_hats : np.ndarray\n",
    "#         Estimated cluster assignments of shape (m, k).\n",
    "    \n",
    "#     gamma_cut : np.ndarray\n",
    "#         Ground truth cluster assignments of shape (m, k).\n",
    "    \n",
    "#     k : int\n",
    "#         Number of clusters.\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     identify : np.ndarray\n",
    "#         Indices of the identified clusters of shape (k,).\n",
    "    \n",
    "#     precision : np.ndarray\n",
    "#         Precision scores for each cluster of shape (k,).\n",
    "    \n",
    "#     recall : np.ndarray\n",
    "#         Recall scores for each cluster of shape (k,).\n",
    "    \n",
    "#     f : np.ndarray\n",
    "#         F1 scores for each cluster of shape (k,).\n",
    "    \n",
    "#     cl_errors : np.ndarray\n",
    "#         Clustering errors for each cluster of shape (k,).\n",
    "    \n",
    "#     NMI_score : np.ndarray\n",
    "#         Normalized Mutual Information scores for each cluster of shape (k,).\n",
    "    \n",
    "#     num_of_edges : np.ndarray\n",
    "#         Number of edges for each cluster of shape (k,).\n",
    "#     \"\"\"\n",
    "#     identify = np.zeros(k, dtype=int)\n",
    "#     cl_err = np.inf * np.ones(k)\n",
    "#     precision = np.zeros(k)\n",
    "#     recall = np.zeros(k)\n",
    "#     f = np.zeros(k)\n",
    "#     NMI_score = np.zeros(k)\n",
    "#     num_of_edges = np.zeros(k)\n",
    "\n",
    "#     for i in range(k):\n",
    "#         W = np.diag(np.diag(Ls[:, :, i])) - Ls[:, :, i]\n",
    "#         W[W < 0.001] = 0\n",
    "#         Ls[:, :, i] = np.diag(np.sum(W, axis=1)) - W\n",
    "#         for j in range(k):\n",
    "#             er = np.linalg.norm(gamma_hats[:, i] - gamma_cut[:, j])\n",
    "#             if cl_err[i] > er:\n",
    "#                 cl_err[i] = er\n",
    "#                 identify[i] = j\n",
    "\n",
    "#     for i in range(k):\n",
    "#         idx = identify[i]\n",
    "#         precision[i], recall[i], f[i], NMI_score[i], num_of_edges[i] = graph_learning_perf_eval(Lap[:, :, idx], Ls[:, :, i])\n",
    "\n",
    "#     # Compute clustering errors\n",
    "#     cl_errors = np.array([np.linalg.norm(gamma_hats[:, i] - gamma_cut[:, identify[i]])**2 for i in range(k)])\n",
    "\n",
    "#     return identify, precision, recall, f, cl_errors, NMI_score, num_of_edges\n",
    "\n",
    "\n",
    "# def graph_learning_perf_eval(L_0, L):\n",
    "#     \"\"\"\n",
    "#     Evaluates the performance of a learned graph by comparing it with the ground truth Laplacian matrix.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     L_0 : np.ndarray\n",
    "#         Ground truth Laplacian matrix of shape (n, n).\n",
    "    \n",
    "#     L : np.ndarray\n",
    "#         Learned Laplacian matrix of shape (n, n).\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     precision : float\n",
    "#         Precision score of the learned graph.\n",
    "    \n",
    "#     recall : float\n",
    "#         Recall score of the learned graph.\n",
    "    \n",
    "#     f : float\n",
    "#         F1 score of the learned graph.\n",
    "    \n",
    "#     NMI_score : float\n",
    "#         Normalized Mutual Information score of the learned graph.\n",
    "    \n",
    "#     num_of_edges : int\n",
    "#         Number of edges in the learned graph.\n",
    "#     \"\"\"\n",
    "#     # Edges in the ground truth graph\n",
    "#     L_0tmp = L_0 - np.diag(np.diag(L_0))\n",
    "#     L_0tmp = (L_0tmp + L_0tmp.T) / 2  \n",
    "#     edges_groundtruth = squareform(L_0tmp) != 0\n",
    "\n",
    "#     # Edges in the learned graph\n",
    "#     Ltmp = L - np.diag(np.diag(L))\n",
    "#     Ltmp = (Ltmp + Ltmp.T) / 2  \n",
    "#     edges_learned = squareform(Ltmp) != 0\n",
    "\n",
    "#     # Compute precision, recall, F1-score\n",
    "#     precision = precision_score(edges_groundtruth.astype(int), edges_learned.astype(int), zero_division=0)\n",
    "#     recall = recall_score(edges_groundtruth.astype(int), edges_learned.astype(int), zero_division=0)\n",
    "#     f = f1_score(edges_groundtruth.astype(int), edges_learned.astype(int), zero_division=0)\n",
    "\n",
    "#     # NMI\n",
    "#     NMI_score = normalized_mutual_info_score(edges_learned.astype(int), edges_groundtruth.astype(int))\n",
    "#     if np.isnan(NMI_score):\n",
    "#         NMI_score = 0\n",
    "\n",
    "#     # Number of edges in the learned graph\n",
    "#     num_of_edges = np.sum(edges_learned)\n",
    "\n",
    "#     return precision, recall, f, NMI_score, num_of_edges\n",
    "\n",
    "# def graph_learning_perf_eval(L_0, L):\n",
    "#     import numpy as np\n",
    "#     from scipy.spatial.distance import squareform\n",
    "#     from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "#     # Edges in the ground truth graph\n",
    "#     L_0tmp = L_0 - np.diag(np.diag(L_0))\n",
    "#     edges_groundtruth = squareform(L_0tmp) != 0\n",
    "\n",
    "#     # Edges in the learned graph\n",
    "#     Ltmp = L - np.diag(np.diag(L))\n",
    "#     edges_learned = squareform(Ltmp) != 0\n",
    "\n",
    "#     # Compute True Positives, False Positives, True Negatives, False Negatives\n",
    "#     TP = np.sum((edges_learned == 1) & (edges_groundtruth == 1))\n",
    "#     FP = np.sum((edges_learned == 1) & (edges_groundtruth == 0))\n",
    "#     FN = np.sum((edges_learned == 0) & (edges_groundtruth == 1))\n",
    "#     TN = np.sum((edges_learned == 0) & (edges_groundtruth == 0))\n",
    "\n",
    "#     # Compute precision and recall from first principles\n",
    "#     if TP + FP > 0:\n",
    "#         precision = TP / (TP + FP)\n",
    "#     else:\n",
    "#         precision = 0.0\n",
    "\n",
    "#     if TP + FN > 0:\n",
    "#         recall = TP / (TP + FN)\n",
    "#     else:\n",
    "#         recall = 0.0\n",
    "\n",
    "#     # Compute F1 score\n",
    "#     if precision + recall > 0:\n",
    "#         f = 2 * precision * recall / (precision + recall)\n",
    "#     else:\n",
    "#         f = 0.0\n",
    "\n",
    "#     # NMI\n",
    "#     NMI_score = normalized_mutual_info_score(edges_learned.astype(int), edges_groundtruth.astype(int))\n",
    "#     if np.isnan(NMI_score):\n",
    "#         NMI_score = 0.0\n",
    "\n",
    "#     # Number of edges in the learned graph\n",
    "#     num_of_edges = np.sum(edges_learned)\n",
    "\n",
    "#     return precision, recall, f, NMI_score, num_of_edges\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k):\n",
    "#     identify = np.zeros(k, dtype=int)\n",
    "#     cl_err = np.inf * np.ones(k)\n",
    "\n",
    "#     # Match clusters by minimal error\n",
    "#     for i in range(k):\n",
    "#         # Make W from L\n",
    "#         W = np.diag(np.diag(Ls[:, :, i])) - Ls[:, :, i]\n",
    "#         W[W < 0.001] = 0\n",
    "#         Ls[:, :, i] = np.diag(np.sum(W, axis=1)) - W\n",
    "\n",
    "#         for j in range(k):\n",
    "#             # In MATLAB: norm(x,'fro') for a vector is just the Euclidean norm.\n",
    "#             # np.linalg.norm default is 2-norm for vectors, which matches MATLAB's 'fro' for vectors.\n",
    "#             er = np.linalg.norm(gamma_hats[:, i] - gamma_cut[:, j])  \n",
    "#             if cl_err[i] > er:\n",
    "#                 cl_err[i] = er\n",
    "#                 identify[i] = j\n",
    "\n",
    "#     precision = np.zeros((k, 1))\n",
    "#     recall = np.zeros((k, 1))\n",
    "#     f = np.zeros((k, 1))\n",
    "\n",
    "#     # For each cluster, evaluate performance\n",
    "#     for i in range(k):\n",
    "#         p, r, ff, NMI_score, num_edges = graph_learning_perf_eval(Lap[:, :, identify[i]], Ls[:, :, i])\n",
    "#         precision[i] = p\n",
    "#         recall[i] = r\n",
    "#         f[i] = ff\n",
    "#         # NMI_score and num_edges are computed but not returned as per MATLAB code.\n",
    "\n",
    "#     # Compute cluster errors\n",
    "#     # (gamma_hats - gamma_cut(:,identify)) gives a (m x k) matrix\n",
    "#     # diag(...) extracts the diagonal of (k x k) matrix = per-cluster error\n",
    "#     diff = gamma_hats - gamma_cut[:, identify]\n",
    "#     cl_errors = np.diag(diff.T @ diff)\n",
    "\n",
    "#     # Return as MATLAB does\n",
    "#     return identify, precision, recall, f, cl_errors\n",
    "\n",
    "\n",
    "# # def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "# #     if params is None:\n",
    "# #         params = {}\n",
    "# #     verbosity = params.get('verbosity', 1)\n",
    "# #     maxit = params.get('maxit', 1000)\n",
    "# #     tol = params.get('tol', 1e-5)\n",
    "# #     step_size = params.get('step_size', 0.5)\n",
    "# #     fix_zeros = params.get('fix_zeros', False)\n",
    "# #     max_w = params.get('max_w', np.inf)\n",
    "\n",
    "# #     w_0 = params.get('w_0', 0)\n",
    "# #     if w_0 != 0 and 'c' not in params:\n",
    "# #         raise ValueError(\"When w_0 is specified, c must also be specified\")\n",
    "# #     c = params.get('c', 0.0 if w_0 == 0 else None)\n",
    "\n",
    "# #     # Convert Z to vector form\n",
    "# #     if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "# #         z = squareform_sp(Z)\n",
    "# #     else:\n",
    "# #         z = Z\n",
    "# #     z = z.ravel()\n",
    "# #     l = len(z)\n",
    "# #     n = int(round((1 + sqrt(1+8*l))/2))\n",
    "\n",
    "# #     if not np.isscalar(w_0):\n",
    "# #         # Convert w_0 to vector form if needed\n",
    "# #         if w_0.ndim == 2 and w_0.shape[0] == w_0.shape[1]:\n",
    "# #             w_0 = squareform_sp(w_0)\n",
    "# #         w_0 = w_0.ravel()\n",
    "# #     else:\n",
    "# #         w_0 = float(w_0)\n",
    "\n",
    "# #     if fix_zeros:\n",
    "# #         edge_mask = params.get('edge_mask', None)\n",
    "# #         if edge_mask is None:\n",
    "# #             raise ValueError(\"edge_mask must be provided when fix_zeros is True\")\n",
    "# #         if edge_mask.ndim == 2 and edge_mask.shape[0] == edge_mask.shape[1]:\n",
    "# #             edge_mask = squareform_sp(edge_mask)\n",
    "# #         edge_mask = edge_mask.ravel()\n",
    "# #         ind = np.flatnonzero(edge_mask)\n",
    "# #         z = z[ind].astype(float)\n",
    "# #         if not np.isscalar(w_0):\n",
    "# #             w_0 = w_0[ind].astype(float)\n",
    "# #     else:\n",
    "# #         z = z.astype(float)\n",
    "# #         if not np.isscalar(w_0):\n",
    "# #             w_0 = w_0.astype(float)\n",
    "\n",
    "# #     w = params.get('W_init', np.zeros_like(z, dtype=float))\n",
    "\n",
    "# #     # Construct S, St\n",
    "# #     if fix_zeros:\n",
    "# #         S, St = sum_squareform(n, edge_mask)\n",
    "# #     else:\n",
    "# #         S, St = sum_squareform(n)\n",
    "\n",
    "# #     K_op = lambda w_: S.dot(w_)\n",
    "# #     Kt_op = lambda z_: St.dot(z_)\n",
    "\n",
    "# #     if fix_zeros:\n",
    "# #         # Estimate norm_K (optional advanced method)\n",
    "# #         x_test = np.random.randn(S.shape[1])\n",
    "# #         for _ in range(5):\n",
    "# #             x_test = St.dot(S.dot(x_test))\n",
    "# #             norm_est = np.linalg.norm(x_test)\n",
    "# #             if norm_est == 0:\n",
    "# #                 break\n",
    "# #             x_test = x_test / norm_est\n",
    "# #         norm_K = sqrt(norm_est)\n",
    "# #     else:\n",
    "# #         norm_K = sqrt(2*(n-1))\n",
    "\n",
    "# #     # Define f, g, h\n",
    "# #     def f_eval(w_):\n",
    "# #         return 2*np.dot(w_, z)\n",
    "# #     def f_prox(w_, c_):\n",
    "# #         return np.minimum(max_w, np.maximum(0, w_ - 2*c_*z))\n",
    "\n",
    "# #     def g_eval(x):\n",
    "# #         return -a * np.sum(np.log(x))\n",
    "# #     def g_prox(z_, c_):\n",
    "# #         return prox_sum_log(z_, c_*a)\n",
    "\n",
    "# #     def g_star_prox(z_, c_):\n",
    "# #         # Ensure z_ is array:\n",
    "# #         z_ = np.asarray(z_, dtype=float)\n",
    "# #         # c_ and a are already floats, so no issue\n",
    "# #         # Ensure prox_sum_log returns a numpy array as well\n",
    "# #         return z_ - c_*a * prox_sum_log(z_/(c_*a), 1/(c_*a))[0]\n",
    "\n",
    "\n",
    "# #     if w_0 == 0:\n",
    "# #         def h_eval(w_):\n",
    "# #             return b*np.sum(w_**2)\n",
    "# #         def h_grad(w_):\n",
    "# #             return 2*b*w_\n",
    "# #         h_beta = 2*b\n",
    "# #     else:\n",
    "# #         def h_eval(w_):\n",
    "# #             return b*np.sum(w_**2) + c*np.sum((w_-w_0)**2)\n",
    "# #         def h_grad(w_):\n",
    "# #             return 2*((b+c)*w_ - c*w_0)\n",
    "# #         h_beta = 2*(b+c)\n",
    "\n",
    "# #     mu = h_beta + norm_K\n",
    "# #     epsilon = lin_map(0.0, [0, 1/(1+mu)], [0,1])\n",
    "# #     gn = lin_map(step_size, [epsilon, (1-epsilon)/mu], [0,1])\n",
    "\n",
    "# #     v_n = K_op(w)\n",
    "\n",
    "# #     stat = {}\n",
    "# #     if verbosity > 1:\n",
    "# #         stat['f_eval'] = np.full(maxit, np.nan)\n",
    "# #         stat['g_eval'] = np.full(maxit, np.nan)\n",
    "# #         stat['h_eval'] = np.full(maxit, np.nan)\n",
    "# #         stat['fgh_eval'] = np.full(maxit, np.nan)\n",
    "# #         stat['pos_violation'] = np.full(maxit, np.nan)\n",
    "# #         print('Relative change of primal, dual variables, and objective fun')\n",
    "\n",
    "# #     import time\n",
    "# #     t0 = time.time()\n",
    "# #     for i in range(maxit):\n",
    "# #         Y_n = w - gn*(h_grad(w) + Kt_op(v_n))\n",
    "# #         y_n = v_n + gn*(K_op(w))\n",
    "\n",
    "# #         P_n = f_prox(Y_n, gn)\n",
    "# #         p_n = g_star_prox(y_n, gn)\n",
    "# #         Q_n = P_n - gn*(h_grad(P_n) + Kt_op(p_n))\n",
    "# #         q_n = p_n + gn*(K_op(P_n))\n",
    "\n",
    "# #         if verbosity > 2:\n",
    "# #             stat['f_eval'][i] = f_eval(w)\n",
    "# #             stat['g_eval'][i] = g_eval(K_op(w))\n",
    "# #             stat['h_eval'][i] = h_eval(w)\n",
    "# #             stat['fgh_eval'][i] = stat['f_eval'][i] + stat['g_eval'][i] + stat['h_eval'][i]\n",
    "# #             stat['pos_violation'][i] = -np.sum(np.minimum(0,w))\n",
    "\n",
    "# #         denom_w = np.linalg.norm(w)\n",
    "# #         if denom_w < 1e-15:\n",
    "# #             denom_w = 1e-15\n",
    "# #         denom_v = np.linalg.norm(v_n)\n",
    "# #         if denom_v < 1e-15:\n",
    "# #             denom_v = 1e-15\n",
    "\n",
    "# #         rel_norm_primal = np.linalg.norm(-Y_n + Q_n)/denom_w\n",
    "# #         rel_norm_dual = np.linalg.norm(-y_n + q_n)/denom_v\n",
    "\n",
    "# #         if verbosity > 3:\n",
    "# #             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   {stat[\"fgh_eval\"][i]:6.3e}')\n",
    "# #         elif verbosity > 2:\n",
    "# #             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   {stat[\"fgh_eval\"][i]:6.3e}')\n",
    "# #         elif verbosity > 1:\n",
    "# #             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}')\n",
    "\n",
    "# #         w = w - Y_n + Q_n\n",
    "# #         v_n = v_n - y_n + q_n\n",
    "\n",
    "# #         if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "# #             break\n",
    "\n",
    "# #     stat['time'] = time.time() - t0\n",
    "# #     if verbosity > 0:\n",
    "# #         obj_val = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "# #         print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {obj_val:6.3e}')\n",
    "# #         print(f'Time needed is {stat[\"time\"]} seconds')\n",
    "\n",
    "# #     if fix_zeros:\n",
    "# #         full_w = np.zeros(l, dtype=float)\n",
    "# #         full_w[ind] = w\n",
    "# #         w = full_w\n",
    "\n",
    "# #     if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "# #         W = squareform_sp(w)\n",
    "# #     else:\n",
    "# #         W = w\n",
    "\n",
    "# #     return W, stat\n",
    "\n",
    "# # ===== TESTING THE FUNCTION =====\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a small test scenario\n",
    "#     np.random.seed(42)\n",
    "#     n = 5\n",
    "#     coords = np.random.rand(n,2)\n",
    "#     Z_dense = np.sum((coords[:,None,:] - coords[None,:,:])**2, axis=2)\n",
    "#     np.fill_diagonal(Z_dense, 0)\n",
    "\n",
    "#     # Attempt to learn the graph\n",
    "#     W, stat = gsp_learn_graph_log_degrees(Z_dense, a=1.0, b=1.0, params={'verbosity':2, 'maxit':200})\n",
    "\n",
    "#     print(\"Learned graph W:\\n\", W.toarray() if hasattr(W, 'toarray') else W)\n",
    "#     if 'f_eval' in stat and not np.all(np.isnan(stat['f_eval'])):\n",
    "#         last_idx = np.where(~np.isnan(stat['f_eval']))[0][-1]\n",
    "#         final_obj = stat['f_eval'][last_idx] + stat['g_eval'][last_idx] + stat['h_eval'][last_idx]\n",
    "#         print(\"Final Objective Value:\", final_obj)\n",
    "\n",
    "\n",
    "\n",
    "# def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "#     \"\"\"\n",
    "#     Learns a graph structure by optimizing a log-degrees model.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     Z : array-like\n",
    "#         Input data matrix or condensed distance matrix (squared pairwise distances).\n",
    "\n",
    "#     a : float\n",
    "#         Coefficient for the logarithmic penalty term (controls connectivity).\n",
    "\n",
    "#     b : float\n",
    "#         Regularization coefficient (controls sparsity).\n",
    "\n",
    "#     params : dict, optional\n",
    "#         A dictionary of optional parameters:\n",
    "#         - 'verbosity': int, level of verbosity (default: 1)\n",
    "#         - 'maxit': int, maximum number of iterations (default: 1000)\n",
    "#         - 'tol': float, tolerance for convergence (default: 1e-5)\n",
    "#         - 'step_size': float, step size for the gradient descent (default: 0.5)\n",
    "#         - 'max_w': float, maximum allowable weight (default: np.inf)\n",
    "#         - 'w_0': array-like or int, initial weight matrix or value (default: 0)\n",
    "#         - 'c': float, regularization coefficient for initial weight matrix\n",
    "#         - 'fix_zeros': bool, whether to fix zeros in the weight matrix (default: False)\n",
    "#         - 'edge_mask': array-like, mask for fixed edges (required if 'fix_zeros' is True)\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     W : array-like\n",
    "#         The learned weight matrix (n x n).\n",
    "\n",
    "#     stat : dict\n",
    "#         Output statistics including convergence information.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If 'params.w_0' is specified but 'params.c' is not, or if 'params.edge_mask' is required but not provided.\n",
    "#     \"\"\"\n",
    "#     # Default parameters\n",
    "#     if params is None:\n",
    "#         params = {}\n",
    "\n",
    "#     verbosity = params.get('verbosity', 1)\n",
    "#     maxit = params.get('maxit', 1000)\n",
    "#     tol = params.get('tol', 1e-5)\n",
    "#     step_size = params.get('step_size', 0.5)\n",
    "#     fix_zeros = params.get('fix_zeros', False)\n",
    "#     max_w = params.get('max_w', np.inf)\n",
    "#     w_0 = params.get('w_0', 0)\n",
    "\n",
    "#     # Initialize z\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         z = Z.copy()\n",
    "#     else:\n",
    "#         z = squareform_sp(Z)\n",
    "\n",
    "#     z = z.flatten()\n",
    "#     z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "#     l = len(z)\n",
    "#     n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "#     # Handle w_0\n",
    "#     if not np.isscalar(w_0) or w_0 != 0:\n",
    "#         if 'c' not in params:\n",
    "#             raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "#         else:\n",
    "#             c = params['c']\n",
    "#         if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "#             w_0 = w_0.copy()\n",
    "#         else:\n",
    "#             w_0 = squareform_sp(w_0)\n",
    "#         w_0 = w_0.flatten()\n",
    "#         w_0 = np.asarray(w_0, dtype=float)\n",
    "#     else:\n",
    "#         w_0 = 0\n",
    "#         c = 0  # Ensure c is defined\n",
    "\n",
    "#     # Handle fix_zeros\n",
    "#     if fix_zeros:\n",
    "#         if 'edge_mask' not in params:\n",
    "#             raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "#         edge_mask = params['edge_mask']\n",
    "#         if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "#             edge_mask = squareform_sp(edge_mask)\n",
    "#         ind = np.nonzero(edge_mask.flatten())[0]\n",
    "#         z = z[ind]\n",
    "#         if not np.isscalar(w_0):\n",
    "#             w_0 = w_0[ind]\n",
    "#     else:\n",
    "#         edge_mask = None\n",
    "\n",
    "#     # Initialize w\n",
    "#     w = np.zeros_like(z)\n",
    "\n",
    "#     # Needed operators\n",
    "#     if fix_zeros:\n",
    "#         S, St = sum_squareform(n, edge_mask)\n",
    "#     else:\n",
    "#         S, St = sum_squareform(n)\n",
    "\n",
    "#     K_op = lambda w: S @ w\n",
    "#     Kt_op = lambda z: St @ z\n",
    "\n",
    "#     if fix_zeros:\n",
    "#         norm_K = sparse_norm(S, ord=2)\n",
    "#     else:\n",
    "#         norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "#     # Define functions f, g, h\n",
    "#     f_eval = lambda w: 2 * np.dot(w, z)\n",
    "#     f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "#     param_prox_log = {'verbose': verbosity - 3}\n",
    "#     g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "#     # g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     # g_star_prox = lambda z_in, c_in: np.asarray(z_in) - c_in * a * prox_sum_log(np.asarray(z_in) / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     g_star_prox = lambda z, c: z - c * a * prox_sum_log(z / (c * a), 1 / (c * a), param_prox_log)[0]\n",
    "\n",
    "\n",
    "#     # Corrected h_eval and h_grad with division by 2\n",
    "#     if np.all(w_0 == 0):\n",
    "#         # No prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2\n",
    "#         h_grad = lambda w: b * w\n",
    "#         h_beta = b  # Lipschitz constant of h_grad\n",
    "#     else:\n",
    "#         # With prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2 + (c / 2) * np.linalg.norm(w - w_0) ** 2\n",
    "#         h_grad = lambda w: b * w + c * (w - w_0)\n",
    "#         h_beta = b + c  # Lipschitz constant of h_grad\n",
    "\n",
    "#     # Parameters for convergence\n",
    "#     mu = h_beta + norm_K\n",
    "#     epsilon = 1e-6  # A small positive value\n",
    "#     gn = (1 - epsilon) / mu  # Step size in (epsilon, (1 - epsilon)/mu)\n",
    "\n",
    "#     # Initialize variables\n",
    "#     v_n = K_op(w)\n",
    "\n",
    "#     stat = {}\n",
    "#     if verbosity > 1:\n",
    "#         stat['f_eval'] = []\n",
    "#         stat['g_eval'] = []\n",
    "#         stat['h_eval'] = []\n",
    "#         stat['fgh_eval'] = []\n",
    "#         stat['pos_violation'] = []\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Iterative optimization\n",
    "#     for i in range(maxit):\n",
    "#         # Primal and dual updates\n",
    "#         Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "#         y_n = v_n + gn * K_op(w)\n",
    "#         P_n = f_prox(Y_n, gn)\n",
    "#         p_n = g_star_prox(y_n, gn)\n",
    "#         Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "#         q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "#         # Compute relative norms\n",
    "#         rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "#         rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "#         if verbosity > 1:\n",
    "#             # Record statistics\n",
    "#             stat['f_eval'].append(f_eval(w))\n",
    "#             stat['g_eval'].append(g_eval(K_op(w)))\n",
    "#             stat['h_eval'].append(h_eval(w))\n",
    "#             stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "#             stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "#             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "\n",
    "#         # Update variables\n",
    "#         w = w - Y_n + Q_n\n",
    "#         v_n = v_n - y_n + q_n\n",
    "\n",
    "#         # Check convergence\n",
    "#         if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "#             break\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     if verbosity > 0:\n",
    "#         final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "#         print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "#         print(f'Time needed is {total_time} seconds')\n",
    "\n",
    "#     # Reconstruct full weight vector if fix_zeros was used\n",
    "#     if fix_zeros:\n",
    "#         w_full = np.zeros(l)\n",
    "#         w_full[ind] = w\n",
    "#         w = w_full\n",
    "\n",
    "#     # Convert vectorized weights back to matrix form\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         W = w\n",
    "#     else:\n",
    "#         W = squareform(w)\n",
    "\n",
    "#     stat['time'] = total_time\n",
    "\n",
    "#     return W, stat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def glmm(y, iterations, classes, spread=0.1, regul=0.15, norm_par=1.5):\n",
    "#     \"\"\"\n",
    "#     Implements a Graph Laplacian Mixture Model (GLMM) \n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     y : np.ndarray\n",
    "#         The input data matrix of shape (m, n) where m is the number of signals\n",
    "#         and n is the number of features.\n",
    "    \n",
    "#     iterations : int\n",
    "#         The number of iterations for the algorithm.\n",
    "    \n",
    "#     classes : int\n",
    "#         The number of clusters (classes).\n",
    "    \n",
    "#     spread : float, optional\n",
    "#         Spread parameter for initializing the Laplacian matrices (default: 0.1).\n",
    "    \n",
    "#     regul : float, optional\n",
    "#         Regularization parameter for the covariance matrices (default: 0.15).\n",
    "    \n",
    "#     norm_par : float, optional\n",
    "#         Normalization parameter for the distance computation (default: 1.5).\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     L : np.ndarray\n",
    "#         The learned Laplacian matrices of shape (n, n, classes).\n",
    "    \n",
    "#     gamma_hat : np.ndarray\n",
    "#         The cluster probabilities of shape (m, classes).\n",
    "    \n",
    "#     mu : np.ndarray\n",
    "#         The cluster means of shape (n, classes).\n",
    "    \n",
    "#     log_likelihood : np.ndarray\n",
    "#         The log-likelihood values over the iterations.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If the dimensions of the input data are not correct.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     delta = 2\n",
    "#     n = y.shape[1]\n",
    "#     m = y.shape[0]\n",
    "\n",
    "#     # Initialize variables\n",
    "#     L = np.zeros((n, n, classes))\n",
    "#     W = np.zeros((n, n, classes))\n",
    "#     sigma = np.zeros((n-1, n-1, classes))\n",
    "#     mu = np.zeros((n, classes))\n",
    "#     gamma_hat = np.zeros((m, classes))\n",
    "#     p = np.zeros(classes)\n",
    "#     vecl = np.zeros((n, n, classes))\n",
    "#     vall = np.zeros((n, n, classes))\n",
    "#     yl = np.zeros((m, n-1, classes))\n",
    "#     log_likelihood = np.zeros(iterations)\n",
    "\n",
    "#     for cls in range(classes):\n",
    "#         L[:, :, cls] = spread * np.eye(n) - spread / n * np.ones((n, n))\n",
    "#         mu_curr = np.mean(y, axis=0) + np.random.randn(n) * np.std(y, axis=0)\n",
    "#         mu[:, cls] = mu_curr - np.mean(mu_curr)\n",
    "#         p[cls] = 1 / classes\n",
    "\n",
    "#     # Start the algorithm\n",
    "#     for it in range(iterations):\n",
    "#         # Expectation step\n",
    "#         pall = 0\n",
    "#         for cls in range(classes):\n",
    "#             # vecl[:, :, cls], vall[:, :, cls] = eig(L[:, :, cls])\n",
    "#             eig_vals, eig_vecs = np.linalg.eig(L[:, :, cls])\n",
    "#             eig_vecs = np.real(eig_vecs)\n",
    "#             eig_vals = np.real(eig_vals)\n",
    "#             eig_vals[eig_vals < 0] = 1e-5\n",
    "#             vall[:, :, cls] = np.diag(eig_vals)\n",
    "#             vecl[:, :, cls] = eig_vecs\n",
    "#             sigma[:, :, cls] = inv(vall[1:n, 1:n, cls] + regul * np.eye(n-1))  \n",
    "#             sigma[:, :, cls] = (sigma[:, :, cls] + sigma[:, :, cls].T) / 2\n",
    "#             yl[:, :, cls] = (y - mu[:, cls].T) @ vecl[:, 1:n, cls]\n",
    "#             pall += p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls], allow_singular=True)\n",
    "\n",
    "#         # Compute cluster probabilities gamma_hat  \n",
    "#         pall[pall == 0] = 0.1\n",
    "#         for cls in range(classes):\n",
    "#             gamma_hat[:, cls] = (p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls])) / pall\n",
    "\n",
    "#         log_likelihood[it] = np.sum(np.log(pall))\n",
    "\n",
    "#         # Maximization step: update mu, W, and p\n",
    "#         for cls in range(classes):\n",
    "#             mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n",
    "#             # yc = repmat(np.sqrt(gamma_hat[:, cls])[:, np.newaxis], 1, n) * (y - mu[:, cls])\n",
    "#             yc = np.sqrt(gamma_hat[:, cls])[:, np.newaxis] * (y - mu[:, cls])\n",
    "#             Z = gsp_distanz(yc) ** 2\n",
    "#             theta = np.mean(Z) / norm_par\n",
    "#             W_curr = delta * (gsp_learn_graph_log_degrees(Z / theta, 1, 1))[0]\n",
    "#             W[:, :, cls] = W_curr\n",
    "#             p[cls] = np.sum(gamma_hat[:, cls]) / m\n",
    "#             # Compute Ls\n",
    "#             L[:, :, cls] = np.diag(np.sum(W[:, :, cls], axis=1)) - W[:, :, cls]\n",
    "#             W_curr[W_curr < 1e-3] = 0\n",
    "#             W[:, :, cls] = W_curr\n",
    "\n",
    "#     return L, gamma_hat, mu, log_likelihood\n",
    "\n",
    "\n",
    "# # ===== TESTING THE FUNCTION =====\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Create a small test scenario\n",
    "#     np.random.seed(42)\n",
    "#     n = 5\n",
    "#     coords = np.random.rand(n,2)\n",
    "#     Z_dense = np.sum((coords[:,None,:] - coords[None,:,:])**2, axis=2)\n",
    "#     np.fill_diagonal(Z_dense, 0)\n",
    "\n",
    "#     # Attempt to learn the graph\n",
    "#     W, stat = gsp_learn_graph_log_degrees(Z_dense, a=1.0, b=1.0, params={'verbosity':2, 'maxit':200})\n",
    "\n",
    "#     print(\"Learned graph W:\\n\", W.toarray() if hasattr(W, 'toarray') else W)\n",
    "#     if 'f_eval' in stat and not np.all(np.isnan(stat['f_eval'])):\n",
    "#         last_idx = np.where(~np.isnan(stat['f_eval']))[0][-1]\n",
    "#         final_obj = stat['f_eval'][last_idx] + stat['g_eval'][last_idx] + stat['h_eval'][last_idx]\n",
    "#         print(\"Final Objective Value:\", final_obj)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def glmm_matlab(y, iterations, classes, spread=0.1, regul=0.15, norm_par=1.5):\n",
    "    \"\"\"\n",
    "    Python version of the MATLAB function glmm_matlab.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray (m x n)\n",
    "        Data matrix with m samples and n features.\n",
    "    iterations : int\n",
    "        Number of iterations.\n",
    "    classes : int\n",
    "        Number of classes (clusters).\n",
    "    spread : float, optional\n",
    "        Default 0.1\n",
    "    regul : float, optional\n",
    "        Default 0.15\n",
    "    norm_par : float, optional\n",
    "        Default 1.5\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    L : ndarray (n x n x classes)\n",
    "        Graph Laplacians for each class.\n",
    "    gamma_hat : ndarray (m x classes)\n",
    "        Cluster posterior probabilities.\n",
    "    mu : ndarray (n x classes)\n",
    "        Cluster means.\n",
    "    log_likelihood : ndarray (iterations,)\n",
    "        Log-likelihood at each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper constants\n",
    "    delta = 2\n",
    "    m, n = y.shape  # m samples, n features\n",
    "\n",
    "    # Initialize arrays\n",
    "    L = np.zeros((n, n, classes))\n",
    "    W = np.zeros((n, n, classes))\n",
    "    sigma = np.zeros((n-1, n-1, classes))\n",
    "    mu = np.zeros((n, classes))\n",
    "    gamma_hat = np.zeros((m, classes))\n",
    "    p = np.zeros(classes)\n",
    "    vecl = np.zeros((n, n, classes))\n",
    "    vall = np.zeros((n, n, classes))\n",
    "    yl = np.zeros((m, n-1, classes))\n",
    "\n",
    "    # Initialization\n",
    "    for class_idx in range(classes):\n",
    "        L[..., class_idx] = spread * np.eye(n) - (spread / n) * np.ones((n, n))\n",
    "        mu_curr = np.mean(y, axis=0) + np.random.randn(n) * np.std(y, axis=0)\n",
    "        mu_curr = mu_curr - np.mean(mu_curr)\n",
    "        mu[:, class_idx] = mu_curr\n",
    "        p[class_idx] = 1.0 / classes\n",
    "\n",
    "    log_likelihood = np.zeros(iterations)\n",
    "\n",
    "    # Main loop\n",
    "    for it in range(iterations):\n",
    "        # Expectation step\n",
    "        # putting everything in eigenvector space of dim-1\n",
    "        pall = np.zeros(m)\n",
    "\n",
    "        for class_idx in range(classes):\n",
    "            # Eigen decomposition\n",
    "            eigvals, eigvecs = np.linalg.eig(L[..., class_idx])\n",
    "            # In MATLAB, [V,D] = eig(L) returns V as eigenvectors and D diag eigenvalues.\n",
    "            # np.linalg.eig returns eigenvalues as a vector and eigenvectors as columns of eigvecs\n",
    "            # We should ensure sorting if needed, but we'll assume identical behavior as MATLAB.\n",
    "            # Let's store directly:\n",
    "            vecl[..., class_idx] = eigvecs\n",
    "            vall[..., class_idx] = np.diag(eigvals)\n",
    "\n",
    "            # sigma = inv(vall(2:n,2:n,class) + regul*I)\n",
    "            # We take the sub-block from index 1 to end (2:n in MATLAB means skipping the first eigenvalue)\n",
    "            sub_eigvals = eigvals[1:]\n",
    "            Sigma_inv = np.diag(sub_eigvals) + regul * np.eye(n-1)\n",
    "            # inverse\n",
    "            Sigma = np.linalg.inv(Sigma_inv)\n",
    "            # Make symmetric\n",
    "            Sigma = (Sigma + Sigma.T) / 2\n",
    "            sigma[..., class_idx] = Sigma\n",
    "\n",
    "            # yl = (y - mu(:,class)') * vecl(:,2:n,class)\n",
    "            # mu[:,class_idx] is shape (n,)\n",
    "            # y is (m,n), mu is (n,), we want (y - mu') => (m,n)\n",
    "            # vecl(:,2:n,class_idx) => vecl[:,: ,class_idx], take from index 1 to n-1 for columns\n",
    "            Y_centered = y - mu[:, class_idx]\n",
    "            YL = Y_centered @ vecl[:, 1:, class_idx]\n",
    "            yl[..., class_idx] = YL\n",
    "\n",
    "            # pall = pall + p(class) * mvnpdf(yl(:,:,class), zeros(1,n-1), sigma(:,:,class))\n",
    "            # We assume mvnpdf(yl, mean=0, cov=sigma) returns a vector of length m\n",
    "            mvn_val = mvnpdf(YL, np.zeros(n-1), Sigma)\n",
    "            pall += p[class_idx] * mvn_val\n",
    "\n",
    "        # Avoid division by zero\n",
    "        pall[pall == 0] = 0.1\n",
    "\n",
    "        # compute gamma_hat\n",
    "        for class_idx in range(classes):\n",
    "            mvn_val = mvnpdf(yl[..., class_idx], np.zeros(n-1), sigma[..., class_idx])\n",
    "            gamma_hat[:, class_idx] = (p[class_idx] * mvn_val) / pall\n",
    "\n",
    "        # log-likelihood\n",
    "        log_likelihood[it] = np.sum(np.log(pall))\n",
    "\n",
    "        # Maximization step\n",
    "        for class_idx in range(classes):\n",
    "            # mu(:,class) = (gamma_hat(:,class)' * y) / sum(gamma_hat(:,class))\n",
    "            wght = gamma_hat[:, class_idx]\n",
    "            mu[:, class_idx] = (wght @ y) / np.sum(wght)\n",
    "\n",
    "            # yc = sqrt(gamma_hat(:,class)) .* (y - mu(:,class)')\n",
    "            # sqrt(gamma_hat(:,class)) is shape (m,)\n",
    "            # (y - mu[:,class_idx]) is (m,n)\n",
    "            # elementwise: we can do broadcast:\n",
    "            yc = (y - mu[:, class_idx]) * np.sqrt(wght)[:, None]\n",
    "\n",
    "            # Z = gsp_distanz(yc).^2\n",
    "            Z = gsp_distanz(yc)\n",
    "            Z = Z**2\n",
    "\n",
    "            # theta = mean(Z(:))/norm_par\n",
    "            theta = np.mean(Z) / norm_par\n",
    "\n",
    "            # W_curr = delta*gsp_learn_graph_log_degrees(Z./theta, 1, 1)\n",
    "            W_curr, _ = gsp_learn_graph_log_degrees(Z / theta, 1, 1, params={})\n",
    "            W_curr = delta * W_curr\n",
    "\n",
    "            p[class_idx] = np.sum(wght) / m\n",
    "\n",
    "            # Compute L(:,:,class)\n",
    "            # L = diag(sum(W)) - W\n",
    "            W_sum = np.sum(W_curr, axis=1)\n",
    "            L[..., class_idx] = np.diag(W_sum) - W_curr\n",
    "\n",
    "            # W_curr(W_curr<1e-3)=0\n",
    "            W_curr[W_curr < 1e-3] = 0\n",
    "            W[..., class_idx] = W_curr\n",
    "\n",
    "    return L, gamma_hat, mu, log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "# Assume the following functions are defined and behave like in MATLAB:\n",
    "# glmm_matlab(y, iterations, k)\n",
    "# gsp_erdos_renyi(n, p) - returns a graph structure with attribute 'L' for Laplacian\n",
    "# nmi(labels1, labels2)\n",
    "# gsp_distanz(X)\n",
    "# gsp_learn_graph_log_degrees(Z, a, b, params)\n",
    "# mvnpdf(X, mean, cov) - returns probability densities\n",
    "\n",
    "def graph_learning_perf_eval(L_0, L):\n",
    "    # Evaluate the performance of graph learning algorithms\n",
    "    L_0tmp = L_0 - np.diag(np.diag(L_0))\n",
    "    edges_groundtruth = (squareform(L_0tmp) != 0)\n",
    "\n",
    "    Ltmp = L - np.diag(np.diag(L))\n",
    "    edges_learned = (squareform(Ltmp) != 0)\n",
    "\n",
    "    # Compute precision, recall\n",
    "    # True Positive (TP): edge is in groundtruth and learned\n",
    "    TP = np.sum(edges_learned & edges_groundtruth)\n",
    "    FP = np.sum(edges_learned & (~edges_groundtruth))\n",
    "    FN = np.sum((~edges_learned) & edges_groundtruth)\n",
    "\n",
    "    if (TP + FP) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "    if (TP + FN) == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "\n",
    "    # F-measure\n",
    "    if (precision + recall) == 0:\n",
    "        f = 0.0\n",
    "    else:\n",
    "        f = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    # NMI - convert boolean arrays to integers\n",
    "    # NMI expects labels or cluster assignments. We'll treat edges as binary labels.\n",
    "    NMI_score = nmi(edges_learned.astype(int), edges_groundtruth.astype(int))\n",
    "    if np.isnan(NMI_score):\n",
    "        NMI_score = 0.0\n",
    "\n",
    "    # number of edges in the learned graph\n",
    "    num_of_edges = np.sum(edges_learned)\n",
    "\n",
    "    return precision, recall, f, NMI_score, num_of_edges\n",
    "\n",
    "\n",
    "def identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k):\n",
    "    identify = np.zeros(k, dtype=int)\n",
    "    cl_err = np.inf * np.ones(k)\n",
    "    for i in range(k):\n",
    "        # Make W from L\n",
    "        W = np.diag(np.diag(Ls[:, :, i])) - Ls[:, :, i]\n",
    "        W[W < 0.001] = 0\n",
    "        Ls[:, :, i] = np.diag(np.sum(W, axis=1)) - W\n",
    "        # Find best matching cluster j\n",
    "        for j in range(k):\n",
    "            er = np.linalg.norm(gamma_hats[:, i] - gamma_cut[:, j], 'fro')\n",
    "            if cl_err[i] > er:\n",
    "                cl_err[i] = er\n",
    "                identify[i] = j\n",
    "\n",
    "    precision = np.zeros((k, 1))\n",
    "    recall = np.zeros((k, 1))\n",
    "    f = np.zeros((k, 1))\n",
    "    for i in range(k):\n",
    "        p, r, ff, NMI_score, num_edges = graph_learning_perf_eval(Lap[:, :, identify[i]], Ls[:, :, i])\n",
    "        precision[i] = p\n",
    "        recall[i] = r\n",
    "        f[i] = ff\n",
    "        # NMI_score and num_edges are not stored as per original code except NMI in a local var.\n",
    "        # If needed, store them similarly.\n",
    "\n",
    "    cl_errors = np.diag((gamma_hats - gamma_cut[:, identify]) \\\n",
    "                        .T @ (gamma_hats - gamma_cut[:, identify]))\n",
    "    return identify, precision, recall, f, cl_errors\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)\n",
    "    n = 15  # graph size\n",
    "    m = 150 # number of signals\n",
    "    k = 2   # number of clusters\n",
    "    zero_thresh = 1e-3\n",
    "\n",
    "    # generate graphs\n",
    "    # We'll try until we get connected graphs as per MATLAB code\n",
    "    def generate_connected_erdos_renyi(n, p=0.7, zero_thresh=1e-3):\n",
    "        while True:\n",
    "            g = gsp_erdos_renyi(n, p)\n",
    "            eigs = np.sort(np.linalg.eigvalsh(g.L))\n",
    "            # ensuring graph is connected\n",
    "            if eigs[1] > zero_thresh:\n",
    "                return g\n",
    "\n",
    "    g_list = [generate_connected_erdos_renyi(n, 0.7, zero_thresh) for _ in range(k)]\n",
    "\n",
    "    gamma = np.random.rand(m, 1)\n",
    "    gamma_cut = np.zeros((m, k))\n",
    "    dist = 0.5\n",
    "    p_arr = np.linspace(0, 1, k+1)\n",
    "\n",
    "    y = np.zeros((m, n))\n",
    "    true_y = np.zeros((m, n, k))\n",
    "    center = np.zeros((n, k))\n",
    "    gauss = np.zeros((n, n, k))\n",
    "    Lap = np.zeros((n, n, k))\n",
    "\n",
    "    for i in range(k):\n",
    "        gc = np.linalg.pinv(g_list[i].L)\n",
    "        gauss[:, :, i] = (gc + gc.T) / 2\n",
    "        Lap[:, :, i] = g_list[i].L\n",
    "        center[:, i] = dist * np.random.randn(n)\n",
    "        center[:, i] = center[:, i] - np.mean(center[:, i])\n",
    "        # Assign responsibilities\n",
    "        gamma_cut[(p_arr[i]<gamma) & (gamma<=p_arr[i+1]), i] = 1\n",
    "        # Generate data\n",
    "        # mvnrnd(center(:,i), gauss(:,:,i), m)\n",
    "        # means shape: (n,)\n",
    "        # covariance shape: (n,n)\n",
    "        Y_i = np.random.multivariate_normal(center[:, i], gauss[:, :, i], m)\n",
    "        Y_i = Y_i * gamma_cut[:, i][:, None]  # scale by responsibilities\n",
    "        true_y[:, :, i] = Y_i\n",
    "        y = y + Y_i\n",
    "\n",
    "    # train a glmm on data y\n",
    "    iterations = 200\n",
    "    Ls, gamma_hats, mus, log_likelihood = glmm_matlab(y, iterations, k)\n",
    "    print(\"Training done\")\n",
    "\n",
    "    print(np.sum(gamma_hats, axis=0))\n",
    "\n",
    "    identify, precision, recall, f, cl_errors = identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k)\n",
    "    print(\"identify:\", identify)\n",
    "    print(\"precision:\", precision)\n",
    "    print(\"recall:\", recall)\n",
    "    print(\"f:\", f)\n",
    "    print(\"cl_errors:\", cl_errors)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from math import log\n",
    "\n",
    "def gsp_erdos_renyi(n, p):\n",
    "    \"\"\"\n",
    "    Generate an Erdos-Renyi random graph G(n,p).\n",
    "    Returns an object with L as the Laplacian matrix.\n",
    "    \"\"\"\n",
    "    # Create adjacency matrix\n",
    "    # For i < j, draw edge from Bernoulli(p)\n",
    "    # Ensure no self loops and symmetric\n",
    "    A = np.zeros((n, n))\n",
    "    # Random upper-triangular edges\n",
    "    upper = np.triu(np.random.rand(n, n) < p, 1)\n",
    "    A = upper + upper.T  # symmetric adjacency\n",
    "\n",
    "    # Degree matrix\n",
    "    d = np.sum(A, axis=1)\n",
    "    D = np.diag(d)\n",
    "\n",
    "    # Laplacian\n",
    "    L = D - A\n",
    "\n",
    "    # Return a structure with L\n",
    "    class Graph:\n",
    "        pass\n",
    "    g = Graph()\n",
    "    g.L = L\n",
    "    return g\n",
    "\n",
    "def nmi(labels1, labels2):\n",
    "    \"\"\"\n",
    "    Compute the normalized mutual information (NMI) between two label arrays.\n",
    "    labels1 and labels2 are arrays of equal length with discrete assignments.\n",
    "\n",
    "    NMI(X;Y) = 2*I(X;Y) / (H(X) + H(Y))\n",
    "    \"\"\"\n",
    "    labels1 = np.asarray(labels1)\n",
    "    labels2 = np.asarray(labels2)\n",
    "\n",
    "    # Get unique classes and their frequencies\n",
    "    # Joint distribution\n",
    "    unique1 = np.unique(labels1)\n",
    "    unique2 = np.unique(labels2)\n",
    "\n",
    "    # Construct joint frequency table\n",
    "    N = len(labels1)\n",
    "    contingency = np.zeros((len(unique1), len(unique2)))\n",
    "    for i, c1 in enumerate(unique1):\n",
    "        for j, c2 in enumerate(unique2):\n",
    "            contingency[i, j] = np.sum((labels1 == c1) & (labels2 == c2))\n",
    "\n",
    "    # Joint probability\n",
    "    Pxy = contingency / N\n",
    "\n",
    "    # Marginal probabilities\n",
    "    Px = Pxy.sum(axis=1)\n",
    "    Py = Pxy.sum(axis=0)\n",
    "\n",
    "    # Compute entropies\n",
    "    def entropy(prob):\n",
    "        prob = prob[prob > 0]\n",
    "        return -np.sum(prob * np.log(prob))\n",
    "\n",
    "    Hx = entropy(Px)\n",
    "    Hy = entropy(Py)\n",
    "\n",
    "    # Mutual information I(X;Y)\n",
    "    # I(X;Y) = sum_x,y P(x,y)*log(P(x,y)/(P(x)*P(y)))\n",
    "    # Only sum where Pxy > 0\n",
    "    Pxy_nonzero = Pxy[Pxy > 0]\n",
    "    # corresponding Px,Py:\n",
    "    # We need indices from Pxy to match Px,Py\n",
    "    # Easier way: double loop or vectorized log:\n",
    "    I = 0.0\n",
    "    for i in range(len(unique1)):\n",
    "        for j in range(len(unique2)):\n",
    "            if Pxy[i, j] > 0:\n",
    "                I += Pxy[i, j] * np.log(Pxy[i, j] / (Px[i]*Py[j]))\n",
    "\n",
    "    # NMI\n",
    "    denominator = (Hx + Hy)\n",
    "    if denominator == 0:\n",
    "        return 1.0 if I == 0 else 0.0\n",
    "    NMI = 2*I/denominator\n",
    "    return NMI\n",
    "\n",
    "def mvnpdf(X, mean, cov):\n",
    "    \"\"\"\n",
    "    Evaluate multivariate normal pdf.\n",
    "    X: (m, d)\n",
    "    mean: (d,)\n",
    "    cov: (d,d)\n",
    "    Returns pdf values: (m,)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    mean = np.asarray(mean)\n",
    "    cov = np.asarray(cov)\n",
    "    rv = multivariate_normal(mean=mean, cov=cov)\n",
    "    return rv.pdf(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_sedona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
