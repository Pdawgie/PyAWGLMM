{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import specific functions\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.linalg import eig, inv, pinv, eigvals\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.sparse import csr_matrix, random as sparse_random, find, issparse\n",
    "from scipy.stats import multivariate_normal as mvnpdf\n",
    "import time\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.linalg import inv\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.sparse import triu, coo_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, normalized_mutual_info_score\n",
    "from scipy.sparse.linalg import norm as sparse_norm\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "sys.path.append('/Users/paul_reitz/Documents/repos/PyAWGLMM/Smooth_AWGLMM')\n",
    "from scripts.utils import (\n",
    "    visualize_glmm,\n",
    "    graph_learning_perf_eval,\n",
    "    identify_and_compare,\n",
    "    generate_connected_graph,\n",
    "    normest,\n",
    "    lin_map,\n",
    "    squareform_sp,\n",
    "    sum_squareform,\n",
    "    prox_sum_log,\n",
    "    gsp_distanz,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squareform_sp(w):\n",
    "    \"\"\"\n",
    "    Sparse counterpart of numpy's squareform\n",
    "    \n",
    "    Parameters:\n",
    "    w : sparse or dense vector with n(n-1)/2 elements OR matrix with size [n, n] and zero diagonal\n",
    "    \n",
    "    Returns:\n",
    "    W : matrix form of input vector w OR vector form of input matrix W\n",
    "    \"\"\"\n",
    "    if sp.issparse(w):\n",
    "        is_sparse = True\n",
    "    else:\n",
    "        is_sparse = False\n",
    "        w = np.asarray(w)\n",
    "    \n",
    "    # Determine if input is a vector\n",
    "    if w.ndim == 1 or w.shape[0] == 1 or w.shape[1] == 1:\n",
    "        # VECTOR -> MATRIX\n",
    "        l = w.size\n",
    "        n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))\n",
    "        if l != n * (n - 1) // 2:\n",
    "            raise ValueError(\"Bad vector size!\")\n",
    "        \n",
    "        if is_sparse:\n",
    "            ind_vec = w.nonzero()[0]\n",
    "            s = w.data\n",
    "        else:\n",
    "            ind_vec = np.nonzero(w)[0]\n",
    "            s = w[ind_vec]\n",
    "        \n",
    "        num_nz = len(ind_vec)\n",
    "        ind_i = np.zeros(num_nz, dtype=int)\n",
    "        ind_j = np.zeros(num_nz, dtype=int)\n",
    "        \n",
    "        curr_row = 0\n",
    "        offset = 0\n",
    "        len_row = n - 1\n",
    "        for idx in range(num_nz):\n",
    "            ind_vec_i = ind_vec[idx]\n",
    "            while ind_vec_i >= (len_row + offset):\n",
    "                offset += len_row\n",
    "                len_row -= 1\n",
    "                curr_row += 1\n",
    "            ind_i[idx] = curr_row\n",
    "            ind_j[idx] = ind_vec_i - offset + curr_row + 1\n",
    "        \n",
    "        # For the lower triangular part, add the transposed matrix\n",
    "        data = np.concatenate([s, s])\n",
    "        row_indices = np.concatenate([ind_i, ind_j])\n",
    "        col_indices = np.concatenate([ind_j, ind_i])\n",
    "        W = sp.csr_matrix((data, (row_indices, col_indices)), shape=(n, n))\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # MATRIX -> VECTOR\n",
    "        m, n = w.shape\n",
    "        if m != n or not np.all(w.diagonal() == 0):\n",
    "            raise ValueError(\"Matrix has to be square with zero diagonal!\")\n",
    "        \n",
    "        if is_sparse:\n",
    "            # Convert to COO format to align data and indices\n",
    "            w_coo = w.tocoo()\n",
    "            ind_i = w_coo.row\n",
    "            ind_j = w_coo.col\n",
    "            s = w_coo.data\n",
    "        else:\n",
    "            ind_i, ind_j = np.nonzero(w)\n",
    "            s = w[ind_i, ind_j]\n",
    "        \n",
    "        # Keep only upper triangular part\n",
    "        ind_upper = ind_i < ind_j\n",
    "        ind_i = ind_i[ind_upper]\n",
    "        ind_j = ind_j[ind_upper]\n",
    "        s = s[ind_upper]\n",
    "        \n",
    "        # Compute new (vector) index from (i,j) (matrix) indices\n",
    "        new_ind = n * ind_i - ind_i * (ind_i + 1) // 2 + ind_j - ind_i - 1\n",
    "        l = n * (n - 1) // 2\n",
    "        w_vec = sp.csr_matrix((s, (new_ind, np.zeros_like(new_ind))), shape=(l, 1))\n",
    "        return w_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python test\n",
    "# import numpy as np\n",
    "# import scipy.sparse as sp\n",
    "\n",
    "# # Sample symmetric matrix with zeros on the diagonal\n",
    "# n = 5\n",
    "# W = sp.random(n, n, density=0.2)\n",
    "# W = W + W.T  # Make it symmetric\n",
    "# W.setdiag(0)  # Set diagonal to zero\n",
    "\n",
    "# # Convert matrix to vector\n",
    "# w_vec = squareform_sp(W)\n",
    "\n",
    "# # Convert vector back to matrix\n",
    "# W_reconstructed = squareform_sp(w_vec)\n",
    "\n",
    "# # Verify that W and W_reconstructed are the same\n",
    "# print( (W != W_reconstructed).nnz == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squareform(n, mask=None):\n",
    "    import numpy as np\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = np.asarray(mask).flatten()\n",
    "        if len(mask) != n * (n - 1) // 2:\n",
    "            raise ValueError('Mask size has to be n(n-1)/2')\n",
    "\n",
    "        ind_vec = np.flatnonzero(mask)\n",
    "        ncols = len(ind_vec)\n",
    "\n",
    "        I = np.zeros(ncols, dtype=int)\n",
    "        J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "        curr_row = 0\n",
    "        offset = 0\n",
    "        len_row = n - 1\n",
    "        for ii in range(ncols):\n",
    "            ind_vec_i = ind_vec[ii]\n",
    "            while ind_vec_i > (len_row + offset - 1):\n",
    "                offset += len_row\n",
    "                len_row -= 1\n",
    "                curr_row += 1\n",
    "            I[ii] = curr_row\n",
    "            J[ii] = ind_vec_i - offset + curr_row + 1\n",
    "    else:\n",
    "        ncols = n * (n - 1) // 2\n",
    "        I = np.zeros(ncols, dtype=int)\n",
    "        J = np.zeros(ncols, dtype=int)\n",
    "\n",
    "        k = 0\n",
    "        for i in range(n - 1):\n",
    "            for j in range(i + 1, n):\n",
    "                I[k] = i\n",
    "                J[k] = j\n",
    "                k += 1\n",
    "\n",
    "    # Construct St\n",
    "    row_indices = np.concatenate([np.arange(ncols), np.arange(ncols)])\n",
    "    col_indices = np.concatenate([I, J])\n",
    "    data = np.ones(2 * ncols)\n",
    "\n",
    "    St = csr_matrix((data, (row_indices, col_indices)), shape=(ncols, n))\n",
    "    S = St.transpose()\n",
    "\n",
    "    return S, St\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Define parameters\n",
    "n = 4\n",
    "mask = None  # or np.random.randint(0, 2, size=n * (n - 1) // 2)\n",
    "\n",
    "# Generate random weights\n",
    "W = np.random.rand(n, n)\n",
    "W = (W + W.T) / 2  # Make it symmetric\n",
    "np.fill_diagonal(W, 0)\n",
    "\n",
    "# Vectorize W using squareform\n",
    "from scipy.spatial.distance import squareform\n",
    "w = squareform(W)\n",
    "\n",
    "# Get S and St\n",
    "S, St = sum_squareform(n, mask)\n",
    "\n",
    "# Compute sum of weights connected to each node\n",
    "node_sums = S @ w\n",
    "\n",
    "# Compare with actual sums\n",
    "actual_sums = W.sum(axis=1)\n",
    "\n",
    "# Verify they are close\n",
    "print( np.allclose(node_sums, actual_sums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "#     \"\"\"\n",
    "#     Learns a graph structure by optimizing a log-degrees model.\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     Z : array-like\n",
    "#         Input data matrix or condensed distance matrix (squared pairwise distances).\n",
    "\n",
    "#     a : float\n",
    "#         Coefficient for the logarithmic penalty term (controls connectivity).\n",
    "\n",
    "#     b : float\n",
    "#         Regularization coefficient (controls sparsity).\n",
    "\n",
    "#     params : dict, optional\n",
    "#         A dictionary of optional parameters:\n",
    "#         - 'verbosity': int, level of verbosity (default: 1)\n",
    "#         - 'maxit': int, maximum number of iterations (default: 1000)\n",
    "#         - 'tol': float, tolerance for convergence (default: 1e-5)\n",
    "#         - 'step_size': float, step size for the gradient descent (default: 0.5)\n",
    "#         - 'max_w': float, maximum allowable weight (default: np.inf)\n",
    "#         - 'w_0': array-like or int, initial weight matrix or value (default: 0)\n",
    "#         - 'c': float, regularization coefficient for initial weight matrix\n",
    "#         - 'fix_zeros': bool, whether to fix zeros in the weight matrix (default: False)\n",
    "#         - 'edge_mask': array-like, mask for fixed edges (required if 'fix_zeros' is True)\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     W : array-like\n",
    "#         The learned weight matrix (n x n).\n",
    "\n",
    "#     stat : dict\n",
    "#         Output statistics including convergence information.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If 'params.w_0' is specified but 'params.c' is not, or if 'params.edge_mask' is required but not provided.\n",
    "#     \"\"\"\n",
    "#     # Default parameters\n",
    "#     if params is None:\n",
    "#         params = {}\n",
    "\n",
    "#     verbosity = params.get('verbosity', 1)\n",
    "#     maxit = params.get('maxit', 1000)\n",
    "#     tol = params.get('tol', 1e-5)\n",
    "#     step_size = params.get('step_size', 0.5)\n",
    "#     fix_zeros = params.get('fix_zeros', False)\n",
    "#     max_w = params.get('max_w', np.inf)\n",
    "#     w_0 = params.get('w_0', 0)\n",
    "\n",
    "#     # Initialize z\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         z = Z.copy()\n",
    "#     else:\n",
    "#         z = squareform_sp(Z)\n",
    "\n",
    "#     z = z.toarray().flatten()\n",
    "#     z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "#     l = len(z)\n",
    "#     n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "#     # Handle w_0\n",
    "#     if not np.isscalar(w_0) or w_0 != 0:\n",
    "#         if 'c' not in params:\n",
    "#             raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "#         else:\n",
    "#             c = params['c']\n",
    "#         if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "#             w_0 = w_0.copy()\n",
    "#         else:\n",
    "#             w_0 = squareform_sp(w_0)\n",
    "#         w_0 = w_0.flatten()\n",
    "#         w_0 = np.asarray(w_0, dtype=float)\n",
    "#     else:\n",
    "#         w_0 = 0\n",
    "#         c = 0  # Ensure c is defined\n",
    "\n",
    "#     # Handle fix_zeros\n",
    "#     if fix_zeros:\n",
    "#         if 'edge_mask' not in params:\n",
    "#             raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "#         edge_mask = params['edge_mask']\n",
    "#         if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "#             edge_mask = squareform_sp(edge_mask)\n",
    "#         ind = np.nonzero(edge_mask.flatten())[0]\n",
    "#         z = z[ind]\n",
    "#         if not np.isscalar(w_0):\n",
    "#             w_0 = w_0[ind]\n",
    "#     else:\n",
    "#         edge_mask = None\n",
    "\n",
    "#     # Initialize w\n",
    "#     w = np.zeros_like(z)\n",
    "\n",
    "#     # Needed operators\n",
    "#     if fix_zeros:\n",
    "#         S, St = sum_squareform(n, edge_mask)\n",
    "#     else:\n",
    "#         S, St = sum_squareform(n)\n",
    "\n",
    "#     K_op = lambda w: S @ w\n",
    "#     Kt_op = lambda z: St @ z\n",
    "\n",
    "#     if fix_zeros:\n",
    "#         norm_K = sparse_norm(S, ord=2)\n",
    "#     else:\n",
    "#         norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "#     # Define functions f, g, h\n",
    "#     f_eval = lambda w: 2 * np.dot(w, z)\n",
    "#     f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "#     param_prox_log = {'verbose': verbosity - 3}\n",
    "#     g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "#     # g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     # g_star_prox = lambda z_in, c_in: np.asarray(z_in) - c_in * a * prox_sum_log(np.asarray(z_in) / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "#     g_star_prox = lambda z, c: z - c * a * prox_sum_log(z / (c * a), 1 / (c * a), param_prox_log)[0]\n",
    "\n",
    "\n",
    "#     # Corrected h_eval and h_grad with division by 2\n",
    "#     if np.all(w_0 == 0):\n",
    "#         # No prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2\n",
    "#         h_grad = lambda w: b * w\n",
    "#         h_beta = b  # Lipschitz constant of h_grad\n",
    "#     else:\n",
    "#         # With prior W0\n",
    "#         h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2 + (c / 2) * np.linalg.norm(w - w_0) ** 2\n",
    "#         h_grad = lambda w: b * w + c * (w - w_0)\n",
    "#         h_beta = b + c  # Lipschitz constant of h_grad\n",
    "\n",
    "#     # Parameters for convergence\n",
    "#     mu = h_beta + norm_K\n",
    "#     epsilon = 1e-6  # A small positive value\n",
    "#     gn = (1 - epsilon) / mu  # Step size in (epsilon, (1 - epsilon)/mu)\n",
    "\n",
    "#     # Initialize variables\n",
    "#     v_n = K_op(w)\n",
    "\n",
    "#     stat = {}\n",
    "#     if verbosity > 1:\n",
    "#         stat['f_eval'] = []\n",
    "#         stat['g_eval'] = []\n",
    "#         stat['h_eval'] = []\n",
    "#         stat['fgh_eval'] = []\n",
    "#         stat['pos_violation'] = []\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Iterative optimization\n",
    "#     for i in range(maxit):\n",
    "#         # Primal and dual updates\n",
    "#         Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "#         y_n = v_n + gn * K_op(w)\n",
    "#         P_n = f_prox(Y_n, gn)\n",
    "#         p_n = g_star_prox(y_n, gn)\n",
    "#         Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "#         q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "#         # Compute relative norms\n",
    "#         rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "#         rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "#         if verbosity > 1:\n",
    "#             # Record statistics\n",
    "#             stat['f_eval'].append(f_eval(w))\n",
    "#             stat['g_eval'].append(g_eval(K_op(w)))\n",
    "#             stat['h_eval'].append(h_eval(w))\n",
    "#             stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "#             stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "#             print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "\n",
    "#         # Update variables\n",
    "#         w = w - Y_n + Q_n\n",
    "#         v_n = v_n - y_n + q_n\n",
    "\n",
    "#         # Check convergence\n",
    "#         if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "#             break\n",
    "\n",
    "#     total_time = time.time() - start_time\n",
    "#     if verbosity > 0:\n",
    "#         final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "#         print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "#         print(f'Time needed is {total_time} seconds')\n",
    "\n",
    "#     # Reconstruct full weight vector if fix_zeros was used\n",
    "#     if fix_zeros:\n",
    "#         w_full = np.zeros(l)\n",
    "#         w_full[ind] = w\n",
    "#         w = w_full\n",
    "\n",
    "#     # Convert vectorized weights back to matrix form\n",
    "#     if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "#         W = w\n",
    "#     else:\n",
    "#         W = squareform(w)\n",
    "\n",
    "#     stat['time'] = total_time\n",
    "\n",
    "#     return W, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def squareform_sp(Z):\n",
    "#     \"\"\"\n",
    "#     Converts a distance matrix Z into a vector form (or vice versa) similar to MATLAB's squareform,\n",
    "#     but returns a sparse matrix.\n",
    "#     \"\"\"\n",
    "#     if isinstance(Z, np.ndarray):\n",
    "#         # Assuming Z is a square matrix, extract the upper triangular part excluding the diagonal\n",
    "#         triu_indices = np.triu_indices_from(Z, k=1)\n",
    "#         data = Z[triu_indices]\n",
    "#         return coo_matrix((data, triu_indices), shape=Z.shape)\n",
    "#     else:\n",
    "#         # If Z is already a vector, return it as is\n",
    "#         return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "\n",
    "    # Default parameters\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    verbosity = params.get('verbosity', 1)\n",
    "    maxit = params.get('maxit', 1000)\n",
    "    tol = params.get('tol', 1e-5)\n",
    "    step_size = params.get('step_size', 0.5)  # Should be in (0, 1)\n",
    "    fix_zeros = params.get('fix_zeros', False)\n",
    "    max_w = params.get('max_w', np.inf)\n",
    "    w_0 = params.get('w_0', 0)\n",
    "\n",
    "    if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "        z = Z.copy()\n",
    "    else:\n",
    "        z = squareform_sp(Z)\n",
    "    \n",
    "    # Ensure z is a dense NumPy array\n",
    "    if isinstance(z, np.ndarray):\n",
    "        z = z.flatten()\n",
    "    else:\n",
    "        z = z.toarray().flatten()\n",
    "        \n",
    "    z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "    l = len(z)\n",
    "    n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "    # Handle w_0\n",
    "    if not np.isscalar(w_0) or w_0 != 0:\n",
    "        if 'c' not in params:\n",
    "            raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "        else:\n",
    "            c = params['c']\n",
    "        if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "            w_0 = w_0.copy()\n",
    "        else:\n",
    "            w_0 = squareform_sp(w_0)\n",
    "        w_0 = w_0.flatten()\n",
    "        w_0 = np.asarray(w_0, dtype=float)\n",
    "    else:\n",
    "        w_0 = 0\n",
    "        c = 0  # Ensure c is defined\n",
    "\n",
    "    # Handle fix_zeros\n",
    "    if fix_zeros:\n",
    "        if 'edge_mask' not in params:\n",
    "            raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "        edge_mask = params['edge_mask']\n",
    "        if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "            edge_mask = squareform_sp(edge_mask)\n",
    "        ind = np.nonzero(edge_mask.flatten())[0]\n",
    "        z = z[ind]\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0[ind]\n",
    "    else:\n",
    "        edge_mask = None\n",
    "\n",
    "    # Initialize w\n",
    "    w = np.zeros_like(z)\n",
    "\n",
    "    # Needed operators\n",
    "    if fix_zeros:\n",
    "        S, St = sum_squareform(n, edge_mask)\n",
    "    else:\n",
    "        S, St = sum_squareform(n)\n",
    "\n",
    "    K_op = lambda w: S @ w\n",
    "    Kt_op = lambda z: St @ z\n",
    "\n",
    "    if fix_zeros:\n",
    "        norm_K = sparse_norm(S, ord=2)\n",
    "    else:\n",
    "        norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "    # Define functions f, g, h\n",
    "    f_eval = lambda w: 2 * np.dot(w, z)\n",
    "    f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "    param_prox_log = {'verbose': verbosity - 3}\n",
    "    g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "    g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)[0]\n",
    "\n",
    "    # Corrected h_eval and h_grad\n",
    "    if np.all(w_0 == 0):\n",
    "        # No prior W0\n",
    "        h_eval = lambda w: b * np.linalg.norm(w) ** 2\n",
    "        h_grad = lambda w: 2 * b * w\n",
    "        h_beta = 2 * b\n",
    "    else:\n",
    "        # With prior W0\n",
    "        h_eval = lambda w: b * np.linalg.norm(w) ** 2 + c * np.linalg.norm(w - w_0) ** 2\n",
    "        h_grad = lambda w: 2 * b * w + 2 * c * (w - w_0)\n",
    "        h_beta = 2 * (b + c)\n",
    "\n",
    "    # Parameters for convergence\n",
    "    mu = h_beta + norm_K\n",
    "    gn = step_size / mu  # Step size in (0, 1/mu)\n",
    "\n",
    "    # Initialize variables\n",
    "    v_n = K_op(w)\n",
    "\n",
    "    stat = {}\n",
    "    if verbosity > 1:\n",
    "        stat['f_eval'] = []\n",
    "        stat['g_eval'] = []\n",
    "        stat['h_eval'] = []\n",
    "        stat['fgh_eval'] = []\n",
    "        stat['pos_violation'] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterative optimization\n",
    "    for i in range(maxit):\n",
    "        # Primal and dual updates\n",
    "        Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "        y_n = v_n + gn * K_op(w)\n",
    "        P_n = f_prox(Y_n, gn)\n",
    "        p_n = g_star_prox(y_n, gn)\n",
    "        Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "        q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "        # Compute relative norms\n",
    "        rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "        rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "        if verbosity > 1:\n",
    "            # Record statistics\n",
    "            stat['f_eval'].append(f_eval(w))\n",
    "            stat['g_eval'].append(g_eval(K_op(w)))\n",
    "            stat['h_eval'].append(h_eval(w))\n",
    "            stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "            stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "            if verbosity > 2:\n",
    "                print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "            else:\n",
    "                print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}')\n",
    "\n",
    "        # Update variables\n",
    "        w = w - Y_n + Q_n\n",
    "        v_n = v_n - y_n + q_n\n",
    "\n",
    "        # Check convergence\n",
    "        if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    if verbosity > 0:\n",
    "        final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "        print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "        print(f'Time needed is {total_time:.4f} seconds')\n",
    "\n",
    "    # Reconstruct full weight vector if fix_zeros was used\n",
    "    if fix_zeros:\n",
    "        w_full = np.zeros(l)\n",
    "        w_full[ind] = w\n",
    "        w = w_full\n",
    "\n",
    "    # Convert vectorized weights back to matrix form\n",
    "    if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "        W = w\n",
    "    else:\n",
    "        W = squareform(w)\n",
    "\n",
    "    stat['time'] = total_time\n",
    "\n",
    "    return W, stat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "    \"\"\"\n",
    "    Python version of the MATLAB function gsp_learn_graph_log_degrees.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    verbosity = params.get('verbosity', 1)\n",
    "    maxit = params.get('maxit', 1000)\n",
    "    tol = params.get('tol', 1e-5)\n",
    "    step_size = params.get('step_size', 0.5)\n",
    "    fix_zeros = params.get('fix_zeros', False)\n",
    "    max_w = params.get('max_w', np.inf)\n",
    "\n",
    "    # Handle w_0 and c if provided\n",
    "    w_0 = params.get('w_0', None)\n",
    "    c = params.get('c', 0.0 if w_0 is None else None)\n",
    "    if w_0 is not None and c is None:\n",
    "        raise ValueError('When params.w_0 is specified, params.c should also be specified')\n",
    "    \n",
    "    # Convert Z to the vector form z\n",
    "    if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "        # Z is n-by-n\n",
    "        z = squareform_sp(Z)\n",
    "    else:\n",
    "        # Z is a vector\n",
    "        z = Z\n",
    "    z = z.toarray().flatten()\n",
    "    l = len(z)\n",
    "    n = int(round((1 + np.sqrt(1+8*l))/2))\n",
    "\n",
    "    if w_0 is None:\n",
    "        w_0 = 0\n",
    "        c = 0.0\n",
    "    else:\n",
    "        if w_0.ndim == 2 and w_0.shape[0] == w_0.shape[1]:\n",
    "            w_0 = squareform_sp(w_0)\n",
    "        w_0 = w_0.flatten()\n",
    "\n",
    "    # If fix_zeros is set, use only a subset of edges\n",
    "    if fix_zeros:\n",
    "        edge_mask = params.get('edge_mask', None)\n",
    "        if edge_mask is None:\n",
    "            raise ValueError('edge_mask must be provided when fix_zeros is True')\n",
    "        if edge_mask.ndim == 2 and edge_mask.shape[0] == edge_mask.shape[1]:\n",
    "            edge_mask = squareform_sp(edge_mask)\n",
    "        edge_mask = edge_mask.flatten()\n",
    "        ind = np.where(edge_mask != 0)[0]\n",
    "        z = z[ind]\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0[ind]\n",
    "    else:\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0\n",
    "        else:\n",
    "            w_0 = np.array([w_0]*l)\n",
    "\n",
    "    z = z.astype(float)\n",
    "    w_0 = w_0.astype(float) if w_0.size == l else w_0\n",
    "    w = np.zeros_like(z)\n",
    "\n",
    "    # Construct operators S and St\n",
    "    if fix_zeros:\n",
    "        S, St = sum_squareform(n, edge_mask)\n",
    "    else:\n",
    "        S, St = sum_squareform(n)\n",
    "\n",
    "    # Operator K_op = S*w and Kt_op = St*z\n",
    "    def K_op(w_):\n",
    "        return S.dot(w_)\n",
    "    def Kt_op(z_):\n",
    "        return St.dot(z_)\n",
    "\n",
    "    # Norm estimate of K\n",
    "    # If fix_zeros: norm_K is approximated by normest(S)\n",
    "    # Otherwise norm_K = sqrt(2*(n-1))\n",
    "    if fix_zeros:\n",
    "        # normest: a rough estimation can be done by power method:\n",
    "        # For simplicity, we do a quick estimate\n",
    "        # (This is a minor approximation; for full fidelity, implement a proper normest.)\n",
    "        # We'll just do a few power iterations:\n",
    "        x = np.random.randn(S.shape[1])\n",
    "        for _ in range(5):\n",
    "            x = St.dot(S.dot(x))\n",
    "            norm_est = np.linalg.norm(x)\n",
    "            if norm_est == 0:\n",
    "                break\n",
    "            x /= norm_est\n",
    "        norm_K = np.sqrt(norm_est)\n",
    "    else:\n",
    "        norm_K = np.sqrt(2*(n-1))\n",
    "\n",
    "    # Define function handles f, g, h\n",
    "    # f: deals with data fidelity: f.eval = 2*w'z, f.prox = ...\n",
    "    def f_eval(w_):\n",
    "        return 2*np.dot(w_, z)\n",
    "    def f_prox(w_, c_):\n",
    "        return np.minimum(max_w, np.maximum(0, w_ - 2*c_*z))\n",
    "\n",
    "    param_prox_log = {}  # not used extensively here\n",
    "    def g_eval(x):\n",
    "        return -a * np.sum(np.log(x))\n",
    "    def g_prox(z_, c_):\n",
    "        # prox of g w.r.t. z_ at parameter c: g(z) = -a sum(log(z))\n",
    "        # prox_sum_log solves the subproblem coordinate-wise\n",
    "        return prox_sum_log(z_, c_*a, param_prox_log)\n",
    "\n",
    "    # g_star_prox used in the algorithm:\n",
    "    # g_star_prox(z,c) = z - c*a * prox_sum_log(z/(c*a), 1/(c*a))\n",
    "    def g_star_prox(z_, c_):\n",
    "        return z_ - c_*a * prox_sum_log(z_/(c_*a), 1/(c_*a), param_prox_log)\n",
    "\n",
    "    if c == 0:\n",
    "        def h_eval(w_):\n",
    "            return b * np.sum(w_**2)\n",
    "        def h_grad(w_):\n",
    "            return 2*b*w_\n",
    "        h_beta = 2*b\n",
    "    else:\n",
    "        def h_eval(w_):\n",
    "            return b*np.sum(w_**2) + c*np.sum((w_-w_0)**2)\n",
    "        def h_grad(w_):\n",
    "            return 2*((b+c)*w_ - c*w_0)\n",
    "        h_beta = 2*(b+c)\n",
    "\n",
    "    mu = h_beta + norm_K\n",
    "    epsilon = lin_map(0.0, [0, 1/(1+mu)], [0,1])\n",
    "    gn = lin_map(step_size, [epsilon, (1-epsilon)/mu], [0,1])\n",
    "\n",
    "    stat = {}\n",
    "    if verbosity > 1 or True:\n",
    "        stat['f_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['g_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['h_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['fgh_eval'] = np.zeros(maxit)*np.nan\n",
    "        stat['pos_violation'] = np.zeros(maxit)*np.nan\n",
    "\n",
    "    v_n = K_op(w)\n",
    "\n",
    "    import time\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i in range(maxit):\n",
    "        Y_n = w - gn*(h_grad(w) + Kt_op(v_n))\n",
    "        y_n = v_n + gn*(K_op(w))\n",
    "\n",
    "        P_n = f_prox(Y_n, gn)\n",
    "        p_n = g_star_prox(y_n, gn)\n",
    "\n",
    "        Q_n = P_n - gn*(h_grad(P_n) + Kt_op(p_n))\n",
    "        q_n = p_n + gn*(K_op(P_n))\n",
    "\n",
    "        if verbosity > 2 or True:\n",
    "            stat['f_eval'][i] = f_eval(w)\n",
    "            Kw = K_op(w)\n",
    "            stat['g_eval'][i] = g_eval(Kw)\n",
    "            stat['h_eval'][i] = h_eval(w)\n",
    "            stat['fgh_eval'][i] = stat['f_eval'][i] + stat['g_eval'][i] + stat['h_eval'][i]\n",
    "            stat['pos_violation'][i] = -np.sum(np.minimum(0,w))\n",
    "\n",
    "        rel_norm_primal = np.linalg.norm(- Y_n + Q_n)/max(1e-15, np.linalg.norm(w))\n",
    "        rel_norm_dual = np.linalg.norm(- y_n + q_n)/max(1e-15, np.linalg.norm(v_n))\n",
    "\n",
    "        if verbosity > 2:\n",
    "            print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   {stat[\"fgh_eval\"][i]:6.3e}')\n",
    "        elif verbosity > 1:\n",
    "            print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}')\n",
    "\n",
    "        w = w - Y_n + Q_n\n",
    "        v_n = v_n - y_n + q_n\n",
    "\n",
    "        if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "            break\n",
    "\n",
    "    stat['time'] = time.time() - t0\n",
    "    if verbosity > 0:\n",
    "        print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {f_eval(w) + g_eval(K_op(w)) + h_eval(w):6.3e}')\n",
    "        print(f'Time needed is {stat[\"time\"]} seconds')\n",
    "\n",
    "    # Rebuild full w if fix_zeros\n",
    "    if fix_zeros:\n",
    "        # We must put the learned edges back into a vector of length l with zeros where edges are masked out.\n",
    "        full_w = np.zeros(l + len(ind)*0, dtype=float)  # original l from Z\n",
    "        full_w[ind] = w\n",
    "        w = full_w\n",
    "\n",
    "    # Convert w back to W if Z was a matrix\n",
    "    if Z.ndim == 2 and Z.shape[0] == Z.shape[1]:\n",
    "        W = squareform_sp(w)\n",
    "    else:\n",
    "        W = w\n",
    "\n",
    "    return W, stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def glmm(y, iterations, classes, spread=0.1, regul=0.15, norm_par=1.5):\n",
    "#     \"\"\"\n",
    "#     Implements a Graph Laplacian Mixture Model (GLMM) \n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     y : np.ndarray\n",
    "#         The input data matrix of shape (m, n) where m is the number of signals\n",
    "#         and n is the number of features.\n",
    "    \n",
    "#     iterations : int\n",
    "#         The number of iterations for the algorithm.\n",
    "    \n",
    "#     classes : int\n",
    "#         The number of clusters (classes).\n",
    "    \n",
    "#     spread : float, optional\n",
    "#         Spread parameter for initializing the Laplacian matrices (default: 0.1).\n",
    "    \n",
    "#     regul : float, optional\n",
    "#         Regularization parameter for the covariance matrices (default: 0.15).\n",
    "    \n",
    "#     norm_par : float, optional\n",
    "#         Normalization parameter for the distance computation (default: 1.5).\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     L : np.ndarray\n",
    "#         The learned Laplacian matrices of shape (n, n, classes).\n",
    "    \n",
    "#     gamma_hat : np.ndarray\n",
    "#         The cluster probabilities of shape (m, classes).\n",
    "    \n",
    "#     mu : np.ndarray\n",
    "#         The cluster means of shape (n, classes).\n",
    "    \n",
    "#     log_likelihood : np.ndarray\n",
    "#         The log-likelihood values over the iterations.\n",
    "\n",
    "#     Raises:\n",
    "#     -------\n",
    "#     ValueError\n",
    "#         If the dimensions of the input data are not correct.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     delta = 2\n",
    "#     n = y.shape[1]\n",
    "#     m = y.shape[0]\n",
    "\n",
    "#     # Initialize variables\n",
    "#     L = np.zeros((n, n, classes))\n",
    "#     W = np.zeros((n, n, classes))\n",
    "#     sigma = np.zeros((n-1, n-1, classes))\n",
    "#     mu = np.zeros((n, classes))\n",
    "#     gamma_hat = np.zeros((m, classes))\n",
    "#     p = np.zeros(classes)\n",
    "#     vecl = np.zeros((n, n, classes))\n",
    "#     vall = np.zeros((n, n, classes))\n",
    "#     yl = np.zeros((m, n-1, classes))\n",
    "#     log_likelihood = np.zeros(iterations)\n",
    "\n",
    "#     for cls in range(classes):\n",
    "#         L[:, :, cls] = spread * np.eye(n) - spread / n * np.ones((n, n))\n",
    "#         mu_curr = np.mean(y, axis=0) + np.random.randn(n) * np.std(y, axis=0)\n",
    "#         mu[:, cls] = mu_curr - np.mean(mu_curr)\n",
    "#         p[cls] = 1 / classes\n",
    "\n",
    "#     # Start the algorithm\n",
    "#     for it in range(iterations):\n",
    "#         # Expectation step\n",
    "#         pall = 0\n",
    "#         for cls in range(classes):\n",
    "#             # vecl[:, :, cls], vall[:, :, cls] = eig(L[:, :, cls])\n",
    "#             eig_vals, eig_vecs = np.linalg.eig(L[:, :, cls])\n",
    "#             eig_vecs = np.real(eig_vecs)\n",
    "#             eig_vals = np.real(eig_vals)\n",
    "#             eig_vals[eig_vals < 0] = 1e-5\n",
    "#             vall[:, :, cls] = np.diag(eig_vals)\n",
    "#             vecl[:, :, cls] = eig_vecs\n",
    "#             sigma[:, :, cls] = inv(vall[1:n, 1:n, cls] + regul * np.eye(n-1))  \n",
    "#             sigma[:, :, cls] = (sigma[:, :, cls] + sigma[:, :, cls].T) / 2\n",
    "#             yl[:, :, cls] = (y - mu[:, cls].T) @ vecl[:, 1:n, cls]\n",
    "#             pall += p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls], allow_singular=True)\n",
    "\n",
    "#         # Compute cluster probabilities gamma_hat  \n",
    "#         pall[pall == 0] = 0.1\n",
    "#         for cls in range(classes):\n",
    "#             gamma_hat[:, cls] = (p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls])) / pall\n",
    "\n",
    "#         log_likelihood[it] = np.sum(np.log(pall))\n",
    "\n",
    "#         # Maximization step: update mu, W, and p\n",
    "#         for cls in range(classes):\n",
    "#             mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n",
    "#             # yc = repmat(np.sqrt(gamma_hat[:, cls])[:, np.newaxis], 1, n) * (y - mu[:, cls])\n",
    "#             yc = np.sqrt(gamma_hat[:, cls])[:, np.newaxis] * (y - mu[:, cls])\n",
    "#             Z = gsp_distanz(yc) ** 2\n",
    "#             theta = np.mean(Z) / norm_par\n",
    "#             W_curr = delta * (gsp_learn_graph_log_degrees(Z / theta, 1, 1))[0]\n",
    "#             W[:, :, cls] = W_curr\n",
    "#             p[cls] = np.sum(gamma_hat[:, cls]) / m\n",
    "#             # Compute Ls\n",
    "#             L[:, :, cls] = np.diag(np.sum(W[:, :, cls], axis=1)) - W[:, :, cls]\n",
    "#             W_curr[W_curr < 1e-3] = 0\n",
    "#             W[:, :, cls] = W_curr\n",
    "\n",
    "#     return L, gamma_hat, mu, log_likelihood\n",
    "\n",
    "# #   mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gsp_learn_graph_log_degrees(Z, a, b, params=None):\n",
    "    \"\"\"\n",
    "    Learns a graph structure by optimizing a log-degrees model.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    Z : array-like\n",
    "        Input data matrix or condensed distance matrix (squared pairwise distances).\n",
    "\n",
    "    a : float\n",
    "        Coefficient for the logarithmic penalty term (controls connectivity).\n",
    "\n",
    "    b : float\n",
    "        Regularization coefficient (controls sparsity).\n",
    "\n",
    "    params : dict, optional\n",
    "        A dictionary of optional parameters:\n",
    "        - 'verbosity': int, level of verbosity (default: 1)\n",
    "        - 'maxit': int, maximum number of iterations (default: 1000)\n",
    "        - 'tol': float, tolerance for convergence (default: 1e-5)\n",
    "        - 'step_size': float, step size for the gradient descent (default: 0.5)\n",
    "        - 'max_w': float, maximum allowable weight (default: np.inf)\n",
    "        - 'w_0': array-like or int, initial weight matrix or value (default: 0)\n",
    "        - 'c': float, regularization coefficient for initial weight matrix\n",
    "        - 'fix_zeros': bool, whether to fix zeros in the weight matrix (default: False)\n",
    "        - 'edge_mask': array-like, mask for fixed edges (required if 'fix_zeros' is True)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    W : array-like\n",
    "        The learned weight matrix (n x n).\n",
    "\n",
    "    stat : dict\n",
    "        Output statistics including convergence information.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If 'params.w_0' is specified but 'params.c' is not, or if 'params.edge_mask' is required but not provided.\n",
    "    \"\"\"\n",
    "    # Default parameters\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    verbosity = params.get('verbosity', 1)\n",
    "    maxit = params.get('maxit', 1000)\n",
    "    tol = params.get('tol', 1e-5)\n",
    "    step_size = params.get('step_size', 0.5)\n",
    "    fix_zeros = params.get('fix_zeros', False)\n",
    "    max_w = params.get('max_w', np.inf)\n",
    "    w_0 = params.get('w_0', 0)\n",
    "\n",
    "    # Initialize z\n",
    "    if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "        z = Z.copy()\n",
    "    else:\n",
    "        z = squareform_sp(Z)\n",
    "\n",
    "    z = z.flatten()\n",
    "    z = np.asarray(z, dtype=float)  # Ensure z is a NumPy array of floats\n",
    "    l = len(z)\n",
    "    n = int(round((1 + np.sqrt(1 + 8 * l)) / 2))  # Number of nodes\n",
    "\n",
    "    # Handle w_0\n",
    "    if not np.isscalar(w_0) or w_0 != 0:\n",
    "        if 'c' not in params:\n",
    "            raise ValueError('When params[\"w_0\"] is specified, params[\"c\"] should also be specified.')\n",
    "        else:\n",
    "            c = params['c']\n",
    "        if isinstance(w_0, np.ndarray) and w_0.ndim == 1:\n",
    "            w_0 = w_0.copy()\n",
    "        else:\n",
    "            w_0 = squareform_sp(w_0)\n",
    "        w_0 = w_0.flatten()\n",
    "        w_0 = np.asarray(w_0, dtype=float)\n",
    "    else:\n",
    "        w_0 = 0\n",
    "        c = 0  # Ensure c is defined\n",
    "\n",
    "    # Handle fix_zeros\n",
    "    if fix_zeros:\n",
    "        if 'edge_mask' not in params:\n",
    "            raise ValueError('When params[\"fix_zeros\"] is True, params[\"edge_mask\"] must be provided.')\n",
    "        edge_mask = params['edge_mask']\n",
    "        if not isinstance(edge_mask, np.ndarray) or edge_mask.ndim != 1:\n",
    "            edge_mask = squareform_sp(edge_mask)\n",
    "        ind = np.nonzero(edge_mask.flatten())[0]\n",
    "        z = z[ind]\n",
    "        if not np.isscalar(w_0):\n",
    "            w_0 = w_0[ind]\n",
    "    else:\n",
    "        edge_mask = None\n",
    "\n",
    "    # Initialize w\n",
    "    w = np.zeros_like(z)\n",
    "\n",
    "    # Needed operators\n",
    "    if fix_zeros:\n",
    "        S, St = sum_squareform(n, edge_mask)\n",
    "    else:\n",
    "        S, St = sum_squareform(n)\n",
    "\n",
    "    K_op = lambda w: S @ w\n",
    "    Kt_op = lambda z: St @ z\n",
    "\n",
    "    if fix_zeros:\n",
    "        norm_K = sparse_norm(S, ord=2)\n",
    "    else:\n",
    "        norm_K = np.sqrt(2 * (n - 1))\n",
    "\n",
    "    # Define functions f, g, h\n",
    "    f_eval = lambda w: 2 * np.dot(w, z)\n",
    "    f_prox = lambda w, c: np.minimum(max_w, np.maximum(0, w - 2 * c * z))\n",
    "\n",
    "    param_prox_log = {'verbose': verbosity - 3}\n",
    "    g_eval = lambda s: -a * np.sum(np.log(s))\n",
    "    # g_star_prox = lambda z_in, c_in: z_in - c_in * a * prox_sum_log(z_in / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "    # g_star_prox = lambda z_in, c_in: np.asarray(z_in) - c_in * a * prox_sum_log(np.asarray(z_in) / (c_in * a), 1 / (c_in * a), param_prox_log)\n",
    "    g_star_prox = lambda z, c: z - c * a * prox_sum_log(z / (c * a), 1 / (c * a), param_prox_log)[0]\n",
    "\n",
    "\n",
    "    # Corrected h_eval and h_grad with division by 2\n",
    "    if np.all(w_0 == 0):\n",
    "        # No prior W0\n",
    "        h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2\n",
    "        h_grad = lambda w: b * w\n",
    "        h_beta = b  # Lipschitz constant of h_grad\n",
    "    else:\n",
    "        # With prior W0\n",
    "        h_eval = lambda w: (b / 2) * np.linalg.norm(w) ** 2 + (c / 2) * np.linalg.norm(w - w_0) ** 2\n",
    "        h_grad = lambda w: b * w + c * (w - w_0)\n",
    "        h_beta = b + c  # Lipschitz constant of h_grad\n",
    "\n",
    "    # Parameters for convergence\n",
    "    mu = h_beta + norm_K\n",
    "    epsilon = 1e-6  # A small positive value\n",
    "    gn = (1 - epsilon) / mu  # Step size in (epsilon, (1 - epsilon)/mu)\n",
    "\n",
    "    # Initialize variables\n",
    "    v_n = K_op(w)\n",
    "\n",
    "    stat = {}\n",
    "    if verbosity > 1:\n",
    "        stat['f_eval'] = []\n",
    "        stat['g_eval'] = []\n",
    "        stat['h_eval'] = []\n",
    "        stat['fgh_eval'] = []\n",
    "        stat['pos_violation'] = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterative optimization\n",
    "    for i in range(maxit):\n",
    "        # Primal and dual updates\n",
    "        Y_n = w - gn * (h_grad(w) + Kt_op(v_n))\n",
    "        y_n = v_n + gn * K_op(w)\n",
    "        P_n = f_prox(Y_n, gn)\n",
    "        p_n = g_star_prox(y_n, gn)\n",
    "        Q_n = P_n - gn * (h_grad(P_n) + Kt_op(p_n))\n",
    "        q_n = p_n + gn * K_op(P_n)\n",
    "\n",
    "        # Compute relative norms\n",
    "        rel_norm_primal = np.linalg.norm(-Y_n + Q_n) / (np.linalg.norm(w) + 1e-10)\n",
    "        rel_norm_dual = np.linalg.norm(-y_n + q_n) / (np.linalg.norm(v_n) + 1e-10)\n",
    "\n",
    "        if verbosity > 1:\n",
    "            # Record statistics\n",
    "            stat['f_eval'].append(f_eval(w))\n",
    "            stat['g_eval'].append(g_eval(K_op(w)))\n",
    "            stat['h_eval'].append(h_eval(w))\n",
    "            stat['fgh_eval'].append(stat['f_eval'][-1] + stat['g_eval'][-1] + stat['h_eval'][-1])\n",
    "            stat['pos_violation'].append(-np.sum(np.minimum(0, w)))\n",
    "            print(f'iter {i+1:4d}: {rel_norm_primal:6.4e}   {rel_norm_dual:6.4e}   OBJ {stat[\"fgh_eval\"][-1]:6.3e}')\n",
    "\n",
    "        # Update variables\n",
    "        w = w - Y_n + Q_n\n",
    "        v_n = v_n - y_n + q_n\n",
    "\n",
    "        # Check convergence\n",
    "        if rel_norm_primal < tol and rel_norm_dual < tol:\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    if verbosity > 0:\n",
    "        final_obj = f_eval(w) + g_eval(K_op(w)) + h_eval(w)\n",
    "        print(f'# iters: {i+1:4d}. Rel primal: {rel_norm_primal:6.4e} Rel dual: {rel_norm_dual:6.4e}  OBJ {final_obj:6.3e}')\n",
    "        print(f'Time needed is {total_time} seconds')\n",
    "\n",
    "    # Reconstruct full weight vector if fix_zeros was used\n",
    "    if fix_zeros:\n",
    "        w_full = np.zeros(l)\n",
    "        w_full[ind] = w\n",
    "        w = w_full\n",
    "\n",
    "    # Convert vectorized weights back to matrix form\n",
    "    if isinstance(Z, np.ndarray) and Z.ndim == 1:\n",
    "        W = w\n",
    "    else:\n",
    "        W = squareform(w)\n",
    "\n",
    "    stat['time'] = total_time\n",
    "\n",
    "    return W, stat\n",
    "\n",
    "\n",
    "def glmm(y, iterations, classes, spread=0.1, regul=0.15, norm_par=1.5):\n",
    "    \"\"\"\n",
    "    Implements a Graph Laplacian Mixture Model (GLMM) \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : np.ndarray\n",
    "        The input data matrix of shape (m, n) where m is the number of signals\n",
    "        and n is the number of features.\n",
    "    \n",
    "    iterations : int\n",
    "        The number of iterations for the algorithm.\n",
    "    \n",
    "    classes : int\n",
    "        The number of clusters (classes).\n",
    "    \n",
    "    spread : float, optional\n",
    "        Spread parameter for initializing the Laplacian matrices (default: 0.1).\n",
    "    \n",
    "    regul : float, optional\n",
    "        Regularization parameter for the covariance matrices (default: 0.15).\n",
    "    \n",
    "    norm_par : float, optional\n",
    "        Normalization parameter for the distance computation (default: 1.5).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    L : np.ndarray\n",
    "        The learned Laplacian matrices of shape (n, n, classes).\n",
    "    \n",
    "    gamma_hat : np.ndarray\n",
    "        The cluster probabilities of shape (m, classes).\n",
    "    \n",
    "    mu : np.ndarray\n",
    "        The cluster means of shape (n, classes).\n",
    "    \n",
    "    log_likelihood : np.ndarray\n",
    "        The log-likelihood values over the iterations.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If the dimensions of the input data are not correct.\n",
    "    \"\"\"\n",
    "    \n",
    "    delta = 2\n",
    "    n = y.shape[1]\n",
    "    m = y.shape[0]\n",
    "\n",
    "    # Initialize variables\n",
    "    L = np.zeros((n, n, classes))\n",
    "    W = np.zeros((n, n, classes))\n",
    "    sigma = np.zeros((n-1, n-1, classes))\n",
    "    mu = np.zeros((n, classes))\n",
    "    gamma_hat = np.zeros((m, classes))\n",
    "    p = np.zeros(classes)\n",
    "    vecl = np.zeros((n, n, classes))\n",
    "    vall = np.zeros((n, n, classes))\n",
    "    yl = np.zeros((m, n-1, classes))\n",
    "    log_likelihood = np.zeros(iterations)\n",
    "\n",
    "    for cls in range(classes):\n",
    "        L[:, :, cls] = spread * np.eye(n) - spread / n * np.ones((n, n))\n",
    "        mu_curr = np.mean(y, axis=0) + np.random.randn(n) * np.std(y, axis=0)\n",
    "        mu[:, cls] = mu_curr - np.mean(mu_curr)\n",
    "        p[cls] = 1 / classes\n",
    "\n",
    "    # Start the algorithm\n",
    "    for it in range(iterations):\n",
    "        # Expectation step\n",
    "        pall = 0\n",
    "        for cls in range(classes):\n",
    "            # vecl[:, :, cls], vall[:, :, cls] = eig(L[:, :, cls])\n",
    "            eig_vals, eig_vecs = np.linalg.eig(L[:, :, cls])\n",
    "            eig_vecs = np.real(eig_vecs)\n",
    "            eig_vals = np.real(eig_vals)\n",
    "            eig_vals[eig_vals < 0] = 1e-5\n",
    "            vall[:, :, cls] = np.diag(eig_vals)\n",
    "            vecl[:, :, cls] = eig_vecs\n",
    "            sigma[:, :, cls] = inv(vall[1:n, 1:n, cls] + regul * np.eye(n-1))  \n",
    "            sigma[:, :, cls] = (sigma[:, :, cls] + sigma[:, :, cls].T) / 2\n",
    "            yl[:, :, cls] = (y - mu[:, cls].T) @ vecl[:, 1:n, cls]\n",
    "            pall += p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls], allow_singular=True)\n",
    "\n",
    "        # Compute cluster probabilities gamma_hat  \n",
    "        pall[pall == 0] = 0.1\n",
    "        for cls in range(classes):\n",
    "            gamma_hat[:, cls] = (p[cls] * mvnpdf.pdf(yl[:, :, cls], mean=np.zeros(n-1), cov=sigma[:, :, cls])) / pall\n",
    "\n",
    "        log_likelihood[it] = np.sum(np.log(pall))\n",
    "\n",
    "        # Maximization step: update mu, W, and p\n",
    "        for cls in range(classes):\n",
    "            mu[:, cls] = (gamma_hat[:, cls].T @ y) / np.sum(gamma_hat[:, cls])\n",
    "            # yc = repmat(np.sqrt(gamma_hat[:, cls])[:, np.newaxis], 1, n) * (y - mu[:, cls])\n",
    "            yc = np.sqrt(gamma_hat[:, cls])[:, np.newaxis] * (y - mu[:, cls])\n",
    "            Z = gsp_distanz(yc) ** 2\n",
    "            theta = np.mean(Z) / norm_par\n",
    "            W_curr = delta * (gsp_learn_graph_log_degrees(Z / theta, 1, 1))[0]\n",
    "            W[:, :, cls] = W_curr\n",
    "            p[cls] = np.sum(gamma_hat[:, cls]) / m\n",
    "            # Compute Ls\n",
    "            L[:, :, cls] = np.diag(np.sum(W[:, :, cls], axis=1)) - W[:, :, cls]\n",
    "            W_curr[W_curr < 1e-3] = 0\n",
    "            W[:, :, cls] = W_curr\n",
    "\n",
    "    return L, gamma_hat, mu, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m true_y[:, :, i]\n\u001b[1;32m     28\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m---> 29\u001b[0m Ls, gamma_hats, mus, log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mglmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(gamma_hats, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[0;32mIn[22], line 304\u001b[0m, in \u001b[0;36mglmm\u001b[0;34m(y, iterations, classes, spread, regul, norm_par)\u001b[0m\n\u001b[1;32m    302\u001b[0m Z \u001b[38;5;241m=\u001b[39m gsp_distanz(yc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    303\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(Z) \u001b[38;5;241m/\u001b[39m norm_par\n\u001b[0;32m--> 304\u001b[0m W_curr \u001b[38;5;241m=\u001b[39m delta \u001b[38;5;241m*\u001b[39m (\u001b[43mgsp_learn_graph_log_degrees\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    305\u001b[0m W[:, :, \u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m=\u001b[39m W_curr\n\u001b[1;32m    306\u001b[0m p[\u001b[38;5;28mcls\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(gamma_hat[:, \u001b[38;5;28mcls\u001b[39m]) \u001b[38;5;241m/\u001b[39m m\n",
      "Cell \u001b[0;32mIn[22], line 59\u001b[0m, in \u001b[0;36mgsp_learn_graph_log_degrees\u001b[0;34m(Z, a, b, params)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     z \u001b[38;5;241m=\u001b[39m squareform_sp(Z)\n\u001b[0;32m---> 59\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m()\n\u001b[1;32m     60\u001b[0m z \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(z, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)  \u001b[38;5;66;03m# Ensure z is a NumPy array of floats\u001b[39;00m\n\u001b[1;32m     61\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(z)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 15  \n",
    "m = 150  \n",
    "k = 2\n",
    "zero_thresh = 10e-4\n",
    "\n",
    "g = [generate_connected_graph(n, 0.7, zero_thresh) for _ in range(k)]\n",
    "\n",
    "gamma = np.random.rand(m, 1)\n",
    "gamma_cut = np.zeros((m, k))\n",
    "dist = 0.5\n",
    "p = np.linspace(0, 1, k + 1)\n",
    "y = np.zeros((m, n))\n",
    "true_y = np.zeros((m, n, k))\n",
    "center = np.zeros((n, k))\n",
    "gauss = np.zeros((n, n, k))\n",
    "Lap = np.zeros((n, n, k))\n",
    "\n",
    "for i in range(k):\n",
    "    gc = pinv(g[i])\n",
    "    gauss[:, :, i] = (gc + gc.T) / 2\n",
    "    Lap[:, :, i] = g[i]\n",
    "    center[:, i] = dist * np.random.randn(n)\n",
    "    center[:, i] = center[:, i] - np.mean(center[:, i])\n",
    "    gamma_cut[(p[i] < gamma[:, 0]) & (gamma[:, 0] <= p[i + 1]), i] = 1\n",
    "    true_y[:, :, i] = gamma_cut[:, i][:, np.newaxis] * np.random.multivariate_normal(center[:, i], gauss[:, :, i], m)\n",
    "    y += true_y[:, :, i]\n",
    "\n",
    "iterations = 200\n",
    "Ls, gamma_hats, mus, log_likelihood = glmm(y, iterations, k)\n",
    "print('Training done')\n",
    "\n",
    "print(np.sum(gamma_hats, axis=0))\n",
    "\n",
    "identify, precision, recall, f, cl_errors ,NMI_score , num_of_edges = identify_and_compare(Ls, Lap, gamma_hats, gamma_cut, k)\n",
    "\n",
    "print(\"Identify:\", identify)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F-measure:\", f)\n",
    "print(\"Cluster Errors:\", cl_errors)\n",
    "print('Normalized mutual information', NMI_score)\n",
    "print(\"Number of estimated edges\", num_of_edges)\n",
    "summed_gamma_hats = np.sum(gamma_hats, axis=1)\n",
    "summed_gamma_hats_column = summed_gamma_hats[:, np.newaxis]\n",
    "are_all_elements_one = np.allclose(summed_gamma_hats_column, 1, atol=1e-8)\n",
    "print(\"\\nAre all elements in the colum wise summed gamma_hat equal to 1:\", are_all_elements_one)\n",
    "visualize_glmm(Ls, gamma_hats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_sedona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
